performance ,	0.14285714285714285
It	0.0015340784571553803
of AI	0.0064516129032258064
of semi-supervised	0.0064516129032258064
than	0.0019723865877712033
uses the	1.0
used is	0.05
tradition	0.00021915406530791147
to this	0.010101010101010102
senses a	0.02857142857142857
Word-aligned	0.00021915406530791147
impediments and	1.0
implicit	0.00021915406530791147
` play	0.1
achieve sufficiently	0.5
evidence	0.00043830813061582295
The dogs	0.041666666666666664
words without	0.027777777777777776
human performance	0.1
consists of	0.3333333333333333
method	0.0006574621959237344
exercises	0.0008766162612316459
while	0.0006574621959237344
senses are	0.05714285714285714
all world	0.09090909090909091
case ,	0.5
which words	0.045454545454545456
date .	0.5
users can	1.0
alternative to	1.0
every machine	0.2
use of	0.5555555555555556
are based	0.029411764705882353
easily into	1.0
tasks ,	0.6
training people	0.1111111111111111
8 adjectives	1.0
the 1970s	0.005235602094240838
or minimally	0.03571428571428571
shallow	0.0006574621959237344
the World	0.005235602094240838
57	0.00021915406530791147
of surefire	0.0064516129032258064
language processing	0.2222222222222222
, while	0.012875536480686695
general word-sense	0.5
prone	0.00021915406530791147
clustering word	0.5
manually sense-annotated	0.25
this human	0.2
or even	0.03571428571428571
parameter	0.00021915406530791147
lists	0.00043830813061582295
`` lexical	0.05555555555555555
tagging .	0.14285714285714285
substantial amounts	1.0
Part-of-speech tagging	1.0
difficult .	0.4
metonymic extension	1.0
speech and	0.16666666666666666
WSD is	0.03125
follows :	1.0
supervised method	0.07142857142857142
all this	0.09090909090909091
works	0.00021915406530791147
improve applications	1.0
confident	0.00021915406530791147
2001 -RRB-	1.0
words used	0.027777777777777776
organizers	0.00021915406530791147
both labeled	0.3333333333333333
supervised and	0.07142857142857142
, preparing	0.004291845493562232
almost -RRB-	0.3333333333333333
not types	0.045454545454545456
increasing the	1.0
well	0.0006574621959237344
text lexical	0.1111111111111111
mining the	1.0
being better	0.2
hence	0.00021915406530791147
the quality	0.005235602094240838
annotated corpus	0.3333333333333333
co-occurrence	0.00021915406530791147
closest	0.00021915406530791147
Evidence	0.00021915406530791147
in his	0.011363636363636364
conventional	0.00043830813061582295
techniques that	0.1111111111111111
have also	0.06666666666666667
the seminal	0.005235602094240838
successively	0.00021915406530791147
first	0.0010957703265395574
domains	0.0006574621959237344
a long	0.010869565217391304
Senseval-2	0.0006574621959237344
immediately	0.00021915406530791147
accessible ,	1.0
coincide with	1.0
, using	0.017167381974248927
Vector	0.00021915406530791147
would be	1.0
techniques .	0.1111111111111111
implement simple	1.0
OALD	0.00021915406530791147
One sense	0.5
results	0.0008766162612316459
often be	0.16666666666666666
you	0.00021915406530791147
from	0.0030681569143107606
into account	0.1
immediately adjacent	1.0
can cope	0.038461538461538464
make a	0.25
return of	0.5
's Dictionary	0.14285714285714285
boost	0.00021915406530791147
parts	0.0010957703265395574
Baselines .	1.0
dictionary of	0.14285714285714285
assumed that	1.0
in their	0.022727272727272728
measure of	1.0
realistic	0.00043830813061582295
human -LRB-	0.1
be called	0.03125
of selectional	0.0064516129032258064
create	0.00021915406530791147
inventories .	0.3333333333333333
Therefore	0.00043830813061582295
: ``	0.07692307692307693
fish tones	0.2
equivocation	0.00021915406530791147
name	0.00021915406530791147
eligible if	1.0
state-of-the art	1.0
and Mary	0.023529411764705882
As	0.00021915406530791147
surefire	0.00021915406530791147
some cases	0.09090909090909091
: raw	0.038461538461538464
seed	0.00043830813061582295
systems are	0.0625
kind of	1.0
enough	0.00043830813061582295
strong	0.00021915406530791147
entropy	0.00021915406530791147
, SemEval	0.004291845493562232
and purity	0.011764705882352941
, is	0.012875536480686695
from text	0.07142857142857142
small annotated	0.25
inventory is	0.18181818181818182
-RCB- -RRB-	1.0
each is	0.08333333333333333
WSD ,	0.09375
</s> Probably	0.005847953216374269
can often	0.038461538461538464
where	0.00043830813061582295
because human	0.125
Also ,	1.0
hypothesis	0.00021915406530791147
in context	0.011363636363636364
the sentences	0.005235602094240838
classified as	0.3333333333333333
device	0.00021915406530791147
expected to	1.0
input for	1.0
consumed	0.00021915406530791147
WSD .	0.09375
are .	0.029411764705882353
the oldest	0.005235602094240838
external information	1.0
classifier is	0.5
overcoming	0.00021915406530791147
query	0.00021915406530791147
bass .	0.08333333333333333
format	0.00021915406530791147
`` heads	0.05555555555555555
space	0.00021915406530791147
' or	0.08333333333333333
especially for	0.5
surrounding words	1.0
different senses	0.0625
involved	0.00021915406530791147
Recently ,	1.0
definitions is	0.16666666666666666
ontologies ,	1.0
contains the	1.0
choosing the	1.0
a musical	0.010869565217391304
as data	0.02564102564102564
during	0.00043830813061582295
resources	0.0015340784571553803
every word	0.4
Cambridge Language	1.0
compute	0.00021915406530791147
contains	0.00021915406530791147
as discourse	0.02564102564102564
least in	0.5
or discrete	0.03571428571428571
homograph	0.00043830813061582295
variety of	1.0
to state	0.010101010101010102
corpus-based systems	1.0
approaches	0.0041639272408503175
introduced the	1.0
even on	0.3333333333333333
corpora have	0.1
challenge	0.00021915406530791147
are related	0.029411764705882353
bilingual	0.00043830813061582295
repeats ,	1.0
in	0.01928555774709621
famous 1949	1.0
`	0.0021915406530791147
four conventional	1.0
in recent	0.022727272727272728
-LRB- or	0.06382978723404255
Unstructured :	1.0
possible solution	0.3333333333333333
Corpora	0.00043830813061582295
that one	0.06666666666666667
used .	0.1
given discourse	0.16666666666666666
own	0.00043830813061582295
respectively .	1.0
OALD -RRB-	1.0
involved is	1.0
for each	0.10526315789473684
these resources	0.1111111111111111
1940s ,	1.0
simplest possible	1.0
Sense	0.0006574621959237344
simplest	0.00021915406530791147
of coverage	0.0064516129032258064
Semi-supervised	0.00043830813061582295
adaptation ,	0.5
component	0.00021915406530791147
human ability	0.1
applied to	0.5
ones	0.00021915406530791147
between ``	0.5
determine	0.00021915406530791147
: Senseval-1	0.038461538461538464
systems submitted	0.0625
approaches may	0.05263157894736842
reached	0.00021915406530791147
mother -RRB-	1.0
ability	0.00021915406530791147
, kernel-based	0.004291845493562232
is much	0.012987012987012988
with knowledge	0.05555555555555555
probably	0.0006574621959237344
cooks	0.00021915406530791147
sufficiently rich	0.5
often combine	0.16666666666666666
powerful	0.00021915406530791147
whole corpus	1.0
a sentence	0.010869565217391304
should be	0.6666666666666666
WordNet sense	0.1111111111111111
samples	0.00043830813061582295
words	0.007889546351084813
systems continue	0.0625
computational task	0.125
of evaluation	0.0064516129032258064
96 %	1.0
were	0.0013149243918474688
to work	0.020202020202020204
tree -LRB-	0.3333333333333333
in computational	0.03409090909090909
together in	0.5
is instructive	0.012987012987012988
, sense	0.004291845493562232
down	0.00021915406530791147
method .	0.6666666666666666
is consumed	0.012987012987012988
inventories ,	0.6666666666666666
choose a	0.5
as a	0.10256410256410256
lexical sample	0.3076923076923077
% to	0.1111111111111111
world	0.0010957703265395574
will overcome	0.3333333333333333
More complex	1.0
databases	0.00021915406530791147
choice	0.00021915406530791147
relevance of	1.0
assumption	0.00043830813061582295
serves	0.00043830813061582295
Evaluation	0.00043830813061582295
and hand-coded	0.011764705882352941
work on	0.2
evergreen and	1.0
, since	0.008583690987124463
are correct	0.029411764705882353
were previously	0.16666666666666666
database	0.00021915406530791147
, due	0.004291845493562232
, but	0.04291845493562232
by clustering	0.058823529411764705
when	0.0006574621959237344
below	0.00021915406530791147
their senses	0.125
provide	0.0008766162612316459
and robust	0.011764705882352941
narrow	0.00021915406530791147
, motorcar	0.004291845493562232
`` Jill	0.1111111111111111
other languages	0.1111111111111111
number of	1.0
been researched	0.05
methods reminiscent	0.03333333333333333
are provided	0.029411764705882353
For instance	0.14285714285714285
include	0.0006574621959237344
supervised techniques	0.07142857142857142
, including	0.008583690987124463
including	0.0006574621959237344
or a	0.07142857142857142
that the	0.2
train and	0.3333333333333333
is more	0.025974025974025976
exercises .	0.5
search	0.0006574621959237344
activation research	1.0
pen	0.00021915406530791147
words into	0.05555555555555555
datasets and	1.0
developed	0.00021915406530791147
this problem	0.2
noun	0.00021915406530791147
</s> Initially	0.005847953216374269
of part-of-speech	0.0064516129032258064
depend	0.00021915406530791147
difficult task	0.2
learning	0.0026298487836949377
and sense	0.011764705882352941
Evaluation measures	0.5
Other resources	0.4
indeterminants	0.00021915406530791147
The solution	0.041666666666666664
performed by	0.5
' almost	0.08333333333333333
swept	0.00021915406530791147
analysis and	1.0
discrimination	0.00043830813061582295
them to	0.3333333333333333
Graph-based methods	0.5
AI researchers	0.5
of senses	0.025806451612903226
or until	0.03571428571428571
the potential	0.005235602094240838
using any	0.16666666666666666
parameter optimization	1.0
memorandum on	1.0
again is	1.0
adaptation of	0.5
POS-tagging problem	1.0
results .	0.25
lexicographers are	1.0
were prone	0.16666666666666666
hard	0.00021915406530791147
Lexical	0.00021915406530791147
, be	0.004291845493562232
is then	0.025974025974025976
argue that	1.0
task compared	0.07692307692307693
stoplists	0.00021915406530791147
</s> Before	0.005847953216374269
define common	1.0
untagged	0.00043830813061582295
tested and	0.3333333333333333
sufficiently high	0.5
early days	0.6666666666666666
much	0.0013149243918474688
together or	0.5
systems via	0.0625
corpus to	0.13333333333333333
the musical	0.005235602094240838
tend to	1.0
be a	0.03125
Sense-Tagged	0.00021915406530791147
learning .	0.25
given maximum	0.16666666666666666
are included	0.029411764705882353
Domain-driven disambiguation	1.0
In information	0.07142857142857142
</s> Difficulties	0.005847953216374269
is hoped	0.012987012987012988
Warren Weaver	1.0
organization	0.00021915406530791147
vary differently	0.5
And the	0.5
WSD well	0.03125
had been	0.3333333333333333
approaches and	0.05263157894736842
-LRB- hence	0.02127659574468085
Word	0.00043830813061582295
The art	0.041666666666666664
new	0.0010957703265395574
problem impacts	0.08333333333333333
based	0.0006574621959237344
the song	0.005235602094240838
called accuracy	1.0
with very	0.05555555555555555
</s> By	0.005847953216374269
Jill	0.00043830813061582295
robust	0.00021915406530791147
of texts	0.0064516129032258064
task-dependency	0.00021915406530791147
it is	0.47058823529411764
</s> Alternatively	0.005847953216374269
accuracies from	1.0
Wide Web	1.0
Senseval ,	0.2
</s> On	0.005847953216374269
semantic	0.0008766162612316459
Precision	0.00021915406530791147
methods are	0.1
Yarowsky	0.00043830813061582295
, top	0.004291845493562232
in England	0.011363636363636364
knowledge-based systems	0.16666666666666666
word ``	0.05172413793103448
-LRB- OALD	0.02127659574468085
into	0.0021915406530791147
auto ,	1.0
automatically .	0.25
require	0.00021915406530791147
polysemy -RRB-	0.3333333333333333
evaluation of	0.16666666666666666
provoked appearing	1.0
better ,	0.2
: Web	0.038461538461538464
as an	0.07692307692307693
knowing that	1.0
senses ,	0.08571428571428572
languages .	0.3333333333333333
</s> Baselines	0.005847953216374269
, then	0.008583690987124463
Acquisition	0.00021915406530791147
quality of	1.0
training ,	0.1111111111111111
and a	0.023529411764705882
and just	0.011764705882352941
intermediate	0.00021915406530791147
outperform them	1.0
source of	0.5
frequency sounds	0.4
optimisation of	1.0
do not	0.6666666666666666
procedures ,	1.0
on fine-grained	0.03571428571428571
methods that	0.06666666666666667
meaning distinctions	0.25
Corpora External	0.5
depending on	1.0
the WordNet	0.010471204188481676
of caveats	0.0064516129032258064
arise .	1.0
test	0.0013149243918474688
use the	0.1111111111111111
effort	0.00021915406530791147
lexical knowledge	0.23076923076923078
held every	1.0
in Senseval-2	0.011363636363636364
here :	1.0
consists	0.0006574621959237344
for English	0.05263157894736842
within the	0.5
and decision	0.011764705882352941
the meaning	0.005235602094240838
% .	0.1111111111111111
and recall	0.011764705882352941
infer	0.00021915406530791147
testing systems	0.25
's machine	0.14285714285714285
summary The	1.0
engines implement	0.5
perhaps	0.00021915406530791147
routinely above	1.0
nearby	0.00043830813061582295
much attention	0.16666666666666666
, more	0.004291845493562232
former comprises	0.25
for adopting	0.05263157894736842
different techniques	0.0625
been lower	0.05
different .	0.0625
used when	0.05
there is	0.6666666666666666
generalize from	1.0
knowledge did	0.043478260869565216
SemEval-2007 ,	1.0
, most	0.004291845493562232
retrieval .	0.3333333333333333
specify	0.00021915406530791147
take into	0.3333333333333333
corpora Collocation	0.1
repeats	0.00021915406530791147
compute the	1.0
corpora to	0.2
theoretically not	1.0
senses have	0.02857142857142857
Difficulties Differences	1.0
individuals to	1.0
In machine	0.07142857142857142
problem in	0.08333333333333333
word belongs	0.017241379310344827
but since	0.09090909090909091
This attempt	0.25
A	0.0008766162612316459
transferring	0.00021915406530791147
of providing	0.0064516129032258064
via graph-based	1.0
proposed	0.00021915406530791147
running text	1.0
vary	0.00043830813061582295
senses occur	0.02857142857142857
similar contexts	0.5
sample	0.0013149243918474688
been	0.0043830813061582295
Shallow approaches	1.0
Dictionary	0.0006574621959237344
going has	1.0
close to	1.0
for word	0.02631578947368421
extended ,	0.5
, such	0.017167381974248927
dictionary-based method	0.3333333333333333
to test	0.04040404040404041
three	0.00043830813061582295
the above	0.005235602094240838
Semi-supervised methods	0.5
, making	0.008583690987124463
machine-readable	0.00021915406530791147
and hand-annotating	0.011764705882352941
test sets	0.16666666666666666
to consider	0.010101010101010102
and some	0.011764705882352941
techniques reach	0.1111111111111111
or device	0.03571428571428571
known ,	0.25
a mother	0.010869565217391304
governs	0.00021915406530791147
, modulated	0.004291845493562232
all natural	0.09090909090909091
approaches are	0.05263157894736842
A criticism	0.25
like the	0.2
appearing some	1.0
measures ,	0.25
BNC -RRB-	1.0
once for	1.0
researchers .	0.2
well for	0.3333333333333333
, whereas	0.004291845493562232
away .	1.0
feature selection	0.5
without	0.0006574621959237344
mapped to	1.0
is hard	0.012987012987012988
English	0.0010957703265395574
meanings extended	0.25
not relevant	0.045454545454545456
such	0.0046022353714661405
</s> Other	0.017543859649122806
close	0.00021915406530791147
Developing algorithms	1.0
Senseval\/SemEval	0.00021915406530791147
same target	0.2
manually tagged	0.25
, pen	0.004291845493562232
the use	0.010471204188481676
latter	0.0008766162612316459
variants of	0.5
The success	0.041666666666666664
correctly assigned	1.0
algorithms is	0.1111111111111111
plateau	0.00021915406530791147
overlapping word	1.0
against	0.00021915406530791147
given collocation	0.16666666666666666
two variants	0.2
for disambiguation	0.02631578947368421
There are	1.0
Automatic Acquisition	1.0
, domain	0.008583690987124463
of dominant	0.0064516129032258064
's Thesaurus	0.2857142857142857
successful in	0.25
that return	0.03333333333333333
lexicon	0.00043830813061582295
-LRB- 1960	0.02127659574468085
translate to	1.0
days	0.00043830813061582295
as information	0.02564102564102564
However ,	1.0
animal or	1.0
data which	0.1111111111111111
consider general	0.3333333333333333
often correspond	0.16666666666666666
sensitive	0.00021915406530791147
techniques	0.0019723865877712033
anaphoras	0.00021915406530791147
be	0.007012930089853167
required by	0.5
WSD systems	0.1875
their frequency	0.125
put to	1.0
1970s	0.00021915406530791147
External	0.00021915406530791147
a piece	0.010869565217391304
rivière	0.00021915406530791147
may vary	0.3333333333333333
instructive	0.00021915406530791147
solution to	0.6666666666666666
Structured	0.00021915406530791147
Bar-Hillel -LRB-	1.0
a kind	0.010869565217391304
-RRB- word	0.02127659574468085
those n	0.25
the existence	0.005235602094240838
using Cross-Lingual	0.08333333333333333
exhibit	0.00021915406530791147
a comparative	0.010869565217391304
topics	0.00021915406530791147
, lexical	0.004291845493562232
overlap	0.00021915406530791147
is difficult	0.012987012987012988
words were	0.027777777777777776
specific domains	1.0
is the	0.03896103896103896
extracted from	1.0
systems that	0.0625
set intersection	0.16666666666666666
</s> Here	0.005847953216374269
to organize	0.010101010101010102
train an	0.3333333333333333
difficult	0.0010957703265395574
deciding what	1.0
to memorize	0.010101010101010102
to determine	0.010101010101010102
discreteness -RRB-	0.5
lower than	1.0
defining	0.00021915406530791147
, the	0.04721030042918455
instrument -RRB-	0.8
from words	0.07142857142857142
occurrences using	0.16666666666666666
been reported	0.1
when disambiguating	0.3333333333333333
organizers had	1.0
, of	0.004291845493562232
senses with	0.08571428571428572
models to	1.0
this again	0.2
</s> Word-aligned	0.005847953216374269
application	0.00021915406530791147
Machine-readable	0.00021915406530791147
kinds	0.00021915406530791147
of fish	0.01935483870967742
can vary	0.038461538461538464
least some	0.5
makes an	1.0
disambiguation is	0.1
knowledge and	0.08695652173913043
like	0.0010957703265395574
lower	0.00021915406530791147
It uses	0.14285714285714285
their	0.0017532325224632918
weak .	1.0
difficult since	0.2
three years	0.5
associated	0.00021915406530791147
very laborious	0.14285714285714285
, yet	0.004291845493562232
Margaret	0.00021915406530791147
, when	0.008583690987124463
, data	0.004291845493562232
as WordNet	0.05128205128205128
setting .	1.0
account systems	1.0
, substitutes	0.004291845493562232
agree	0.0006574621959237344
-LRB- WSD	0.02127659574468085
a sampling	0.010869565217391304
is being	0.012987012987012988
are typical	0.029411764705882353
and evaluating	0.011764705882352941
try	0.00021915406530791147
word types	0.017241379310344827
on a	0.17857142857142858
, from	0.004291845493562232
particular homographs	0.5
rules	0.00043830813061582295
the lexical	0.005235602094240838
different divisions	0.0625
included	0.00043830813061582295
classified	0.0006574621959237344
and disagreements	0.011764705882352941
bottleneck .	0.25
lexicography is	1.0
even outperform	0.3333333333333333
since the	0.3333333333333333
systems were	0.1875
one language	0.06666666666666667
test ,	0.16666666666666666
Part-of-speech	0.00021915406530791147
the experimenter	0.005235602094240838
for a	0.10526315789473684
and ambiguities	0.011764705882352941
and so	0.011764705882352941
13 verb	1.0
translation ,	0.75
are difficult	0.029411764705882353
requisite	0.00021915406530791147
is independently	0.012987012987012988
: either	0.038461538461538464
and ,	0.011764705882352941
new classifier	0.2
51.4 %	1.0
sentences :	0.5
the Senseval\/SemEval	0.005235602094240838
a system	0.021739130434782608
the whim	0.005235602094240838
metonymic	0.00021915406530791147
sets ,	0.3333333333333333
examples for	0.2
distinct or	0.25
song	0.00043830813061582295
to the	0.08080808080808081
rival	0.00021915406530791147
decided that	1.0
Recall :	1.0
to make	0.010101010101010102
food ,	1.0
supervised	0.0030681569143107606
and that	0.011764705882352941
window of	1.0
potential to	1.0
primarily determined	0.5
but recently	0.09090909090909091
annotated	0.0006574621959237344
Most people	0.5
hard to	1.0
clearly whether	1.0
are deemed	0.029411764705882353
a task	0.043478260869565216
Other semi-supervised	0.2
, words	0.004291845493562232
-RRB- and	0.02127659574468085
to three	0.010101010101010102
Lesk	0.00043830813061582295
, 13	0.004291845493562232
1998	0.00043830813061582295
of target	0.012903225806451613
word overlap	0.017241379310344827
true	0.00043830813061582295
Machine-readable dictionaries	1.0
bootstrapping approach	0.5
experiments unfair	1.0
attention has	0.5
by defining	0.058823529411764705
the whole	0.005235602094240838
</s> Discreteness	0.005847953216374269
lexical-sample	0.00021915406530791147
and tree	0.011764705882352941
either unlabeled	0.5
assumption that	0.5
well-behaved	0.00021915406530791147
` financial	0.1
chosen according	0.3333333333333333
, for	0.004291845493562232
of	0.033968880122726273
occur	0.00021915406530791147
linguistic or	1.0
which word	0.045454545454545456
Domain-driven	0.00021915406530791147
almost as	0.3333333333333333
algorithms might	0.1111111111111111
obviously	0.00021915406530791147
Common sense	1.0
near	0.00021915406530791147
IR -RRB-	0.5
the very	0.005235602094240838
</s> WordNet	0.011695906432748537
thesaurus method	1.0
of methods	0.0064516129032258064
51.4	0.00021915406530791147
there	0.0006574621959237344
of supervised	0.0064516129032258064
too weak	1.0
whim	0.00021915406530791147
be determined	0.03125
many word	1.0
instance	0.00021915406530791147
-RRB- .	0.40425531914893614
</s> ``	0.005847953216374269
search engines	0.6666666666666666
kept	0.00021915406530791147
to supervised	0.010101010101010102
researchers	0.0010957703265395574
the part	0.005235602094240838
words and	0.027777777777777776
labeling ,	1.0
is relevant	0.012987012987012988
of training	0.01935483870967742
if there	0.2
expected	0.00021915406530791147
and lexical	0.03529411764705882
to rival	0.010101010101010102
: I	0.038461538461538464
English .	0.2
statistical	0.00021915406530791147
Main article	1.0
For example	0.5714285714285714
is inter-judge	0.012987012987012988
they need	0.16666666666666666
is enough	0.012987012987012988
performed .	0.5
</s> Part-of-speech	0.005847953216374269
methods ,	0.1
system makes	0.25
it 's	0.058823529411764705
either	0.00043830813061582295
: Precision	0.038461538461538464
is why	0.012987012987012988
whereas	0.00021915406530791147
n	0.00043830813061582295
semantic similarity	0.25
together	0.00043830813061582295
those	0.0008766162612316459
forays	0.00021915406530791147
it one	0.058823529411764705
engineering technology	1.0
and can	0.011764705882352941
Other	0.0010957703265395574
a type	0.021739130434782608
, combinations	0.004291845493562232
such words	0.047619047619047616
higher	0.00021915406530791147
to coarser-grained	0.010101010101010102
difficult to	0.2
recent research	0.3333333333333333
quantities of	1.0
latter sense	0.25
include different	0.3333333333333333
Automatic	0.00021915406530791147
citation needed	1.0
since senses	0.16666666666666666
in text	0.022727272727272728
assignments made	1.0
using WordNet	0.08333333333333333
a window	0.010869565217391304
homographs achieving	1.0
substitutes can	1.0
, semi-supervised	0.004291845493562232
of always	0.0064516129032258064
some new	0.09090909090909091
on coarse-grained	0.07142857142857142
computer ,	0.2
Supervised	0.0008766162612316459
et	0.00021915406530791147
is used	0.03896103896103896
` mouse	0.1
corpora	0.0021915406530791147
inducing word	1.0
information such	0.125
his famous	1.0
on dictionaries	0.03571428571428571
baselines	0.00021915406530791147
than fine-grained	0.1111111111111111
for information	0.02631578947368421
sea	0.0006574621959237344
trained on	0.5
completely different	0.3333333333333333
international	0.00021915406530791147
sense ,	0.020833333333333332
`` songs	0.05555555555555555
using narrow	0.08333333333333333
caveats .	1.0
less than	1.0
of Roget	0.0064516129032258064
pronouns in	1.0
other -RRB-	0.1111111111111111
most given	0.1
Research Unit	0.5
problem was	0.08333333333333333
agree in	0.3333333333333333
so far	0.25
basses	0.00021915406530791147
is an	0.025974025974025976
successor	0.00021915406530791147
heads	0.00021915406530791147
,	0.05106289721674337
increasing	0.00021915406530791147
Inventories	0.00021915406530791147
memorandum	0.00021915406530791147
task-independent sense	1.0
disambiguation with	0.1
for testing	0.05263157894736842
because of	0.25
selected ,	1.0
Sense Inventories	0.3333333333333333
As in	1.0
making constraints	0.3333333333333333
financial	0.00021915406530791147
performance in	0.2857142857142857
of such	0.012903225806451613
Learner 's	1.0
tree and	0.3333333333333333
; what	0.25
earlier forays	0.5
assignment	0.00021915406530791147
tagging with	0.14285714285714285
WordNet inventory	0.1111111111111111
overcome	0.00021915406530791147
being a	0.2
can so	0.038461538461538464
graph connectivity	1.0
by the	0.17647058823529413
While users	1.0
rule-based and	1.0
selection ,	0.5
is also	0.025974025974025976
evoke and	1.0
clusters and	1.0
a paradigm	0.010869565217391304
Masterman and	1.0
because	0.0017532325224632918
potentially making	0.5
sense tagging	0.020833333333333332
her colleagues	1.0
evaluation exercises	0.16666666666666666
as input	0.02564102564102564
identifying which	1.0
minimally	0.00021915406530791147
judgement	0.00021915406530791147
once	0.00021915406530791147
per discourse	0.5
replaced	0.00021915406530791147
a sufficiently	0.010869565217391304
differently in	1.0
here	0.00021915406530791147
colleagues	0.00021915406530791147
incoherent	0.00021915406530791147
comprehensive	0.00021915406530791147
vary from	0.5
every time	0.2
labels ,	1.0
driven by	1.0
relatedness	0.00021915406530791147
finding	0.00021915406530791147
manually sense-tagged	0.25
collocation '	0.5
comparing	0.00043830813061582295
corpus ,	0.13333333333333333
that by	0.03333333333333333
inventory was	0.2727272727272727
8	0.00021915406530791147
-	0.00043830813061582295
a computational	0.021739130434782608
divided	0.00021915406530791147
bark near	0.5
criticism of	1.0
inter-judge	0.00021915406530791147
input	0.00021915406530791147
hand-annotating corpus	1.0
been proposed	0.05
%	0.0019723865877712033
necessarily	0.00021915406530791147
I am	0.5
potential	0.00021915406530791147
Task	0.00021915406530791147
labeled and	1.0
activation	0.00021915406530791147
one to	0.06666666666666667
-LRB- sound	0.02127659574468085
tasks such	0.2
at least	0.2222222222222222
tagging	0.0015340784571553803
until	0.00043830813061582295
be expected	0.03125
senses and	0.02857142857142857
Two -LRB-	0.5
learning ,	0.08333333333333333
are words	0.029411764705882353
yet simple	1.0
-LCB-	0.00021915406530791147
feature space	0.5
SemEval -LRB-	0.5
and their	0.023529411764705882
SemEval -RRB-	0.5
Yarowsky 's	0.5
above and	0.2
distinctions at	0.1111111111111111
automatically derived	0.25
because they	0.25
systems achieve	0.0625
, it	0.04291845493562232
and summary	0.011764705882352941
semi-supervised	0.0008766162612316459
senses Finally	0.02857142857142857
another -RRB-	1.0
By the	1.0
not eligible	0.045454545454545456
window	0.00021915406530791147
limited domains	0.5
to perform	0.020202020202020204
bass in	0.08333333333333333
it with	0.058823529411764705
different domains	0.0625
due to	1.0
supervised systems	0.14285714285714285
induction Unsupervised	0.25
space .	1.0
adopting a	0.5
the distinct	0.005235602094240838
why	0.00021915406530791147
different methods	0.0625
decided	0.00021915406530791147
the relation	0.005235602094240838
rival the	1.0
two main	0.4
using information	0.08333333333333333
extremely	0.00021915406530791147
prone to	1.0
retrieval	0.0006574621959237344
Senseval\/SemEval competitions	1.0
that unsupervised	0.03333333333333333
successfully	0.00021915406530791147
cope	0.00021915406530791147
saw supervised	1.0
or better	0.03571428571428571
sound -RRB-	1.0
mainly because	0.6666666666666666
meanings from	0.25
was a	0.11764705882352941
inference et	1.0
making	0.0006574621959237344
senses can	0.05714285714285714
of spreading	0.0064516129032258064
query and	1.0
Deep approaches	1.0
the best	0.005235602094240838
Local impediments	1.0
of WSD	0.03225806451612903
four	0.00021915406530791147
graph	0.00021915406530791147
easy	0.00021915406530791147
car ,	0.5
criticism	0.00021915406530791147
been the	0.05
of car	0.0064516129032258064
at present	0.1111111111111111
distinct computational	0.25
should spend	0.3333333333333333
if these	0.2
gives superior	1.0
, part-of-speech	0.004291845493562232
occurrences	0.0013149243918474688
sequence	0.00021915406530791147
training corpus	0.2222222222222222
raw unannotated	0.5
particular	0.00043830813061582295
competition is	0.5
Thesauri	0.00021915406530791147
was that	0.11764705882352941
not a	0.09090909090909091
if bass	0.4
practice	0.00043830813061582295
seem	0.00021915406530791147
a known	0.010869565217391304
is trained	0.012987012987012988
an ambiguous	0.09090909090909091
variance Another	0.5
sources	0.00043830813061582295
, state-of-the	0.004291845493562232
problem takes	0.08333333333333333
such knowledge	0.047619047619047616
training set	0.1111111111111111
-LRB- almost	0.02127659574468085
learning approaches	0.08333333333333333
samples should	0.5
WSD became	0.03125
testing purposes	0.25
-LRB- homograph	0.02127659574468085
</s> Also	0.023391812865497075
must	0.00043830813061582295
, perform	0.008583690987124463
Supervised methods	1.0
and currently	0.011764705882352941
raw corpora	0.5
into the	0.2
divisions of	1.0
of each	0.012903225806451613
shallow approaches	1.0
The latter	0.041666666666666664
principle	0.00021915406530791147
Wide	0.00021915406530791147
perhaps the	1.0
was still	0.058823529411764705
to machine-readable	0.010101010101010102
language -LRB-	0.1111111111111111
level is	0.25
setting	0.00021915406530791147
very different	0.14285714285714285
75 %	1.0
, either	0.004291845493562232
target language	0.3333333333333333
to these	0.010101010101010102
or dictionary-based	0.03571428571428571
different languages	0.0625
computational context	0.125
words around	0.027777777777777776
WSD could	0.03125
MRDs	0.00021915406530791147
bark at	0.5
-LRB- 2007	0.02127659574468085
technology which	1.0
read the	1.0
cross-lingual sense	1.0
Senseval	0.0010957703265395574
, to	0.030042918454935622
WordNet	0.0019723865877712033
Inter-judge	0.00021915406530791147
humans do	0.5
training	0.0019723865877712033
Baselines	0.00021915406530791147
oldest problems	1.0
be disambiguated	0.125
might be	1.0
same	0.0010957703265395574
lexical resources	0.15384615384615385
formulated as	0.5
Therefore ,	1.0
article :	1.0
Wikipedia .	0.5
task than	0.07692307692307693
labeled	0.00021915406530791147
tasks	0.0010957703265395574
, although	0.004291845493562232
purposes ,	0.6666666666666666
in-house	0.00021915406530791147
or annotated	0.03571428571428571
probably is	0.3333333333333333
and humans	0.011764705882352941
computer-related	0.00021915406530791147
conventional meanings	0.5
dictionary	0.0015340784571553803
community	0.00021915406530791147
to WSD	0.030303030303030304
second	0.0006574621959237344
Dictionary of	0.3333333333333333
music sense	0.5
gained much	1.0
only	0.0010957703265395574
applicable	0.00021915406530791147
larger	0.00043830813061582295
them	0.0006574621959237344
-RRB- level	0.02127659574468085
implies disambiguating	1.0
, anaphora	0.004291845493562232
test .	0.16666666666666666
method in	0.3333333333333333
Discreteness of	1.0
form	0.0008766162612316459
et cetera	1.0
independently	0.00021915406530791147
approaches .	0.10526315789473684
kind	0.00021915406530791147
indicator of	1.0
The opposite	0.041666666666666664
accuracy .	0.125
three words	0.5
Douglas	0.00021915406530791147
disambiguated by	0.2
polysemy ,	0.3333333333333333
through computational	1.0
part	0.00043830813061582295
spend their	1.0
semi-supervised techniques	0.25
tagged with	0.3333333333333333
infer cross-lingual	1.0
to French	0.010101010101010102
for low	0.02631578947368421
performance measures	0.14285714285714285
exist for	0.3333333333333333
these competitions	0.1111111111111111
'' :	0.06666666666666667
knowledge .	0.17391304347826086
i.e.	0.00043830813061582295
some methods	0.09090909090909091
knowledge acquisition	0.17391304347826086
Naïve Bayes	1.0
bound	0.00021915406530791147
methods on	0.03333333333333333
are Naïve	0.029411764705882353
: Automatic	0.038461538461538464
observed	0.00021915406530791147
labeling	0.00021915406530791147
performance serves	0.14285714285714285
knowledge involved	0.043478260869565216
, consider	0.004291845493562232
</s> From	0.005847953216374269
as follows	0.02564102564102564
-RRB- ,	0.19148936170212766
host	0.00021915406530791147
training examples	0.2222222222222222
, often	0.004291845493562232
mapping to	1.0
clear if	1.0
induced from	0.3333333333333333
assessed on	1.0
English Senseval	0.2
as	0.008547008547008548
Oxford	0.00021915406530791147
swept through	1.0
same ,	0.2
improves Web	1.0
is different	0.012987012987012988
consider the	0.3333333333333333
of techniques	0.0064516129032258064
these things	0.1111111111111111
topics and	1.0
multiple	0.00021915406530791147
-LRB- 2001	0.02127659574468085
which were	0.045454545454545456
a computer-readable	0.010869565217391304
apply supervised	1.0
, Senseval-3	0.004291845493562232
context that	0.16666666666666666
the need	0.005235602094240838
WSD researchers	0.03125
evaluation campaigns	0.16666666666666666
mainly to	0.3333333333333333
success .	0.5
sense-annotated corpora	0.6666666666666666
as word	0.05128205128205128
cetera .	1.0
results using	0.25
below .	1.0
developers	0.00021915406530791147
between	0.00043830813061582295
human ,	0.1
how all	1.0
with known	0.05555555555555555
sample WSD	0.16666666666666666
methods rely	0.03333333333333333
basses ''	1.0
successor ,	1.0
the sequence	0.005235602094240838
simple graph	0.25
bass has	0.16666666666666666
be kept	0.03125
WordNet has	0.1111111111111111
rich	0.00043830813061582295
task at	0.07692307692307693
divided in	1.0
became a	0.5
bewildering	0.00021915406530791147
matter of	1.0
pen as	1.0
</s> Supervised	0.017543859649122806
of analysis	0.0064516129032258064
open problem	1.0
possible algorithm	0.3333333333333333
bound .	1.0
could translate	0.25
eschew -LRB-	1.0
without some	0.3333333333333333
car is	0.5
automatically extracted	0.25
compared within	0.3333333333333333
like Douglas	0.2
about	0.00021915406530791147
campaigns most	0.5
against those	1.0
superior results	0.5
be classified	0.0625
bank '	0.5
task referred	0.07692307692307693
Identification	0.00021915406530791147
, without	0.004291845493562232
and statistically	0.011764705882352941
word meaning	0.017241379310344827
full range	0.5
return	0.00043830813061582295
etc. -RRB-	0.3333333333333333
computer-readable	0.00021915406530791147
`` you	0.05555555555555555
corpus of	0.2
extension	0.00021915406530791147
minimally supervised	1.0
achieve performance	0.5
is using	0.025974025974025976
research in	0.125
choosing	0.00021915406530791147
saw	0.00021915406530791147
each occurrence	0.08333333333333333
Then ,	1.0
decision	0.00043830813061582295
combinations of	1.0
a difficult	0.010869565217391304
knowledge about	0.043478260869565216
is that	0.025974025974025976
domain-specific	0.00021915406530791147
accurate than	1.0
compared against	0.3333333333333333
has progressed	0.0625
making it	0.6666666666666666
the pair	0.005235602094240838
but disambiguation	0.09090909090909091
called	0.00021915406530791147
-LRB- now	0.02127659574468085
one typically	0.06666666666666667
artificial intelligence	1.0
the knowledge	0.015706806282722512
be confused	0.03125
and its	0.023529411764705882
and ``	0.03529411764705882
knowledge automatically	0.043478260869565216
larger training	1.0
new tasks	0.2
rivière `	1.0
two sentences	0.2
;	0.0008766162612316459
has words	0.0625
problem	0.0026298487836949377
an indicator	0.09090909090909091
high levels	1.0
unsupervised learning	0.3333333333333333
people to	0.5
robust IR	1.0
automatically transferring	0.25
word meanings	0.017241379310344827
comparison purposes	1.0
a possible	0.010869565217391304
am	0.00021915406530791147
on its	0.03571428571428571
dogs bark	0.5
Different dictionaries	1.0
time they	0.5
ambiguous word	1.0
although with	1.0
common sense	0.75
adjacent one	1.0
of tasks	0.0064516129032258064
be divided	0.03125
know common	0.5
effort .	1.0
largest	0.00021915406530791147
if	0.0010957703265395574
competition ,	0.5
text .	0.2222222222222222
' -RRB-	0.16666666666666666
HECTOR database	0.5
and context	0.011764705882352941
simple	0.0008766162612316459
on specific	0.03571428571428571
computer-related writing	1.0
approaches would	0.05263157894736842
and looked	0.011764705882352941
learning techniques	0.08333333333333333
it probably	0.058823529411764705
colleagues ,	1.0
from dictionary-based	0.07142857142857142
, given	0.008583690987124463
</s> Performance	0.005847953216374269
a successively	0.010869565217391304
On	0.00021915406530791147
provide co-occurrence	0.25
-RRB- completely	0.02127659574468085
of having	0.0064516129032258064
coded	0.00021915406530791147
use its	0.1111111111111111
engineering	0.00021915406530791147
sounds ''	0.5
Because	0.00021915406530791147
domains .	1.0
language ,	0.2222222222222222
plateau in	1.0
text are	0.1111111111111111
transferring knowledge	1.0
comparison	0.00021915406530791147
more difficult	0.25
ex	0.00021915406530791147
the occurrences	0.005235602094240838
sense per	0.041666666666666664
be performed	0.03125
then deep	0.2
presence	0.00021915406530791147
not exist	0.045454545454545456
tradition in	1.0
formulated in	0.5
human serves	0.1
understood as	0.5
allows both	1.0
first Senseval	0.2
inventory	0.002410694718387026
speciﬁc	0.00021915406530791147
fine-grained polysemy	0.2
small number	0.25
on which	0.10714285714285714
need	0.0006574621959237344
evaluation ,	0.16666666666666666
long	0.00021915406530791147
testing words	0.25
must be	0.5
IR	0.00043830813061582295
that word	0.03333333333333333
all-words and	0.5
performance	0.0015340784571553803
was not	0.058823529411764705
fishing for	0.6666666666666666
Unstructured	0.00021915406530791147
campaigns have	0.5
existence	0.00021915406530791147
subtask of	1.0
or rive	0.03571428571428571
words sea	0.027777777777777776
down one	1.0
</s> Comparison	0.005847953216374269
discrete	0.00021915406530791147
not always	0.045454545454545456
induced must	0.3333333333333333
not necessarily	0.045454545454545456
-RRB- Also	0.02127659574468085
for part-of-speech	0.02631578947368421
portion of	1.0
much more	0.3333333333333333
similarity	0.00043830813061582295
just consider	0.5
-LRB- including	0.02127659574468085
agreed in	1.0
accuracy	0.0017532325224632918
Unsupervised methods	0.75
considerations .	1.0
choice in	1.0
to fine-grained	0.010101010101010102
1990s .	0.5
lack of	1.0
tree	0.0006574621959237344
: the	0.07692307692307693
problems	0.00021915406530791147
evaluation but	0.08333333333333333
WSD :	0.03125
distinct	0.0008766162612316459
is at	0.012987012987012988
gained	0.00021915406530791147
help	0.00021915406530791147
</s> Then	0.005847953216374269
indicator	0.00021915406530791147
Before the	1.0
only for	0.2
69.0	0.00021915406530791147
, thus	0.004291845493562232
-LRB- i.e.	0.0425531914893617
and reasoning	0.011764705882352941
sentences ,	0.25
be tested	0.0625
Collocation resources	1.0
a comprehensive	0.010869565217391304
preserves	0.00021915406530791147
Before	0.00021915406530791147
any corpus	0.3333333333333333
format ,	1.0
semi-supervised system	0.25
disambiguation was	0.1
diversification of	1.0
glossaries	0.00021915406530791147
resources Other	0.14285714285714285
correspond	0.00021915406530791147
sense that	0.020833333333333332
vector	0.00021915406530791147
</s> They	0.017543859649122806
seed data	1.0
go down	0.5
domain labels	0.5
because the	0.125
distinctions has	0.1111111111111111
rules -LRB-	0.5
an	0.002410694718387026
could not	0.25
required	0.00043830813061582295
and standard	0.011764705882352941
be successfully	0.03125
account	0.00021915406530791147
in evaluation	0.011363636363636364
overlap in	1.0
the music	0.005235602094240838
line of	1.0
90 %	1.0
to infer	0.010101010101010102
which could	0.09090909090909091
bottleneck	0.0008766162612316459
what the	0.5
belongs	0.00021915406530791147
classifier ,	0.25
kept together	1.0
classified into	0.3333333333333333
like The	0.2
disambiguating some	0.2
named baselines	0.5
similarity of	1.0
to know	0.010101010101010102
Advanced Learner	1.0
perform best	0.25
Jill and	1.0
than the	0.2222222222222222
Alternatively ,	1.0
separately	0.00021915406530791147
needed to	0.3333333333333333
starts from	1.0
a coherent	0.010869565217391304
in the	0.29545454545454547
well-behaved semantically	1.0
in only	0.011363636363636364
domain	0.00043830813061582295
</s> Support	0.005847953216374269
experimenter	0.00021915406530791147
on translation	0.03571428571428571
measures are	0.25
co-occurrence information	1.0
enabling	0.00021915406530791147
, first	0.004291845493562232
became	0.00043830813061582295
one is	0.06666666666666667
and in	0.047058823529411764
one dictionary	0.06666666666666667
shifted	0.00021915406530791147
far be	0.5
still not	0.5
Finally	0.00021915406530791147
that are	0.03333333333333333
mapped	0.00021915406530791147
that is	0.03333333333333333
only 85	0.2
employed	0.00021915406530791147
sense distinctions	0.0625
Dictionary -	0.6666666666666666
hence ,	1.0
different WSD	0.0625
recently	0.00043830813061582295
best supervised	0.5
portion	0.00021915406530791147
systems in	0.0625
to 69.0	0.010101010101010102
word has	0.017241379310344827
During	0.00043830813061582295
and dogs	0.011764705882352941
in accuracy	0.011363636363636364
language engineering	0.1111111111111111
steadily	0.00021915406530791147
encoded as	0.5
the query	0.005235602094240838
recent evaluation	0.3333333333333333
as seed	0.02564102564102564
clusters\/senses .	1.0
continue	0.00043830813061582295
word may	0.017241379310344827
of coded	0.0064516129032258064
assign parts	1.0
disambiguating the	0.4
meaning being	0.125
provided	0.00021915406530791147
less	0.00021915406530791147
songs	0.00021915406530791147
i.e. ,	0.5
completely	0.0006574621959237344
2	0.00021915406530791147
-LRB-	0.010300241069471838
component of	1.0
content	0.00021915406530791147
been much	0.05
rely primarily	0.3333333333333333
occurrences .	0.3333333333333333
Probably every	1.0
reported in	0.5
instrument	0.0010957703265395574
not unanimously	0.045454545454545456
, new	0.008583690987124463
set of	0.6666666666666666
of speciﬁc	0.0064516129032258064
common evaluation	0.25
is	0.01687486302870918
as parts	0.02564102564102564
researchers have	0.2
or biased	0.03571428571428571
words which	0.027777777777777776
: Main	0.038461538461538464
</s> Different	0.005847953216374269
that encodes	0.03333333333333333
ever accessible	1.0
as ``	0.05128205128205128
memory-based learning	1.0
success rate	0.5
algorithms use	0.1111111111111111
samples on	0.5
comparative evaluation	1.0
variance	0.00043830813061582295
disambiguation algorithms	0.1
Margaret Masterman	1.0
been extended	0.05
Here	0.00021915406530791147
in distinctions	0.011363636363636364
Also	0.0010957703265395574
version of	1.0
near both	1.0
role labeling	1.0
Probably	0.00021915406530791147
and processing	0.011764705882352941
used to	0.2
than a	0.1111111111111111
: Word	0.038461538461538464
the baseline	0.005235602094240838
synonym sets	1.0
chosen from	0.3333333333333333
selection	0.00043830813061582295
% of	0.1111111111111111
Among these	1.0
</s> Warren	0.005847953216374269
WSD task	0.03125
could	0.0008766162612316459
WSD --	0.03125
art being	0.3333333333333333
world knowledge	1.0
</s> Local	0.005847953216374269
hand	0.00021915406530791147
shown	0.0010957703265395574
that can	0.06666666666666667
has been	0.625
equivocation between	1.0
evidence .	0.5
machine ,	0.125
Performance	0.00021915406530791147
disambiguated .	0.4
inventories	0.0006574621959237344
of identifying	0.0064516129032258064
the assumption	0.005235602094240838
knowledge sources	0.043478260869565216
not be	0.09090909090909091
, comparing	0.004291845493562232
the target	0.010471204188481676
largely	0.00021915406530791147
food	0.00021915406530791147
is relatively	0.012987012987012988
several	0.00021915406530791147
frequency -LRB-	0.2
sense induction	0.08333333333333333
in all	0.011363636363636364
e.g. ,	0.5
related to	0.3333333333333333
of sense-annotated	0.0064516129032258064
research is	0.125
data sets	0.1111111111111111
evaluating	0.00021915406530791147
selected	0.00021915406530791147
extract	0.00021915406530791147
and expensive	0.011764705882352941
, parameter	0.004291845493562232
, a	0.030042918454935622
new algorithms	0.2
avoiding bad	1.0
researchers understood	0.2
the reverse	0.005235602094240838
texts ,	1.0
music	0.00043830813061582295
ex .	1.0
semantic relations	0.25
written	0.00021915406530791147
definitions that	0.16666666666666666
occurrence	0.00021915406530791147
in French	0.011363636363636364
essential to	1.0
Both	0.00021915406530791147
either manually	0.5
machine-readable dictionaries	1.0
result	0.0006574621959237344
They	0.0006574621959237344
meaning	0.0017532325224632918
Senseval-2 -LRB-	0.3333333333333333
recent WSD	0.3333333333333333
The underlying	0.041666666666666664
Research	0.00043830813061582295
, which	0.034334763948497854
, outside	0.004291845493562232
To	0.0006574621959237344
-LRB- MRDs	0.02127659574468085
requires its	0.5
inclusion	0.00021915406530791147
the -LRB-	0.005235602094240838
sets -LRB-	0.3333333333333333
exhibit only	1.0
word frequency	0.017241379310344827
algorithms	0.0019723865877712033
use semi-supervised	0.1111111111111111
, so	0.004291845493562232
successfully used	1.0
comparing methods	0.5
variable and	1.0
lectures	0.00021915406530791147
further away	0.5
Roget 's	1.0
Thus ,	1.0
are four	0.029411764705882353
exercises require	0.25
more recently	0.125
by different	0.058823529411764705
which contains	0.045454545454545456
outperforming	0.00021915406530791147
</s> This	0.023391812865497075
unsupervised methods	0.3333333333333333
were to	0.16666666666666666
other and	0.1111111111111111
85	0.00021915406530791147
a plateau	0.010869565217391304
-LRB- musical	0.02127659574468085
closely	0.00043830813061582295
is barely	0.012987012987012988
test these	0.16666666666666666
perform	0.0008766162612316459
systems two	0.0625
fine-grained	0.0010957703265395574
to replicate	0.010101010101010102
are laborious	0.029411764705882353
traditionally	0.00021915406530791147
adjectives ,	1.0
when mining	0.3333333333333333
it can	0.058823529411764705
avoiding	0.00021915406530791147
coarser-grained	0.00021915406530791147
the question	0.005235602094240838
subtask	0.00021915406530791147
accuracy is	0.125
of river	0.0064516129032258064
already	0.00021915406530791147
tagging In	0.14285714285714285
approaches do	0.05263157894736842
normally work	0.5
but go	0.09090909090909091
on in-house	0.03571428571428571
Web for	0.25
1949	0.00021915406530791147
`` I	0.05555555555555555
optimization	0.00021915406530791147
understood	0.00043830813061582295
selection .	0.5
methods Supervised	0.03333333333333333
progressed	0.00021915406530791147
notion of	1.0
their word	0.125
which are	0.13636363636363635
for WSD	0.05263157894736842
machines have	1.0
cooking basses	1.0
Initially	0.00021915406530791147
to understand	0.010101010101010102
punched-card	0.00021915406530791147
The Lesk	0.041666666666666664
that exist	0.03333333333333333
frequently	0.00021915406530791147
each pair	0.08333333333333333
, ontologies	0.004291845493562232
employed in	1.0
and memory-based	0.011764705882352941
significant	0.00021915406530791147
Unit in	1.0
model	0.00043830813061582295
relevant	0.0006574621959237344
what sense	0.5
context ,	0.16666666666666666
discreteness problem	0.5
relation can	1.0
Machines	0.00021915406530791147
that for	0.03333333333333333
not for	0.045454545454545456
Web ,	0.25
be very	0.03125
have low	0.06666666666666667
SemEval-2007	0.00021915406530791147
a domain-specific	0.010869565217391304
but the	0.09090909090909091
going	0.00021915406530791147
be words	0.03125
same corpus	0.2
the second	0.010471204188481676
higher than	1.0
determined	0.00043830813061582295
is done	0.012987012987012988
Evaluation Comparing	0.5
are usually	0.029411764705882353
clustering	0.00043830813061582295
and the	0.047058823529411764
according to	1.0
observed in	1.0
might	0.00021915406530791147
senses becomes	0.02857142857142857
problem with	0.16666666666666666
put	0.00021915406530791147
unlabeled	0.00043830813061582295
true :	0.5
initial	0.00021915406530791147
of manually	0.01935483870967742
of natural	0.0064516129032258064
of different	0.0064516129032258064
involve disambiguating	1.0
ways .	1.0
Approaches	0.00021915406530791147
say clearly	1.0
sounds	0.00043830813061582295
supplements the	1.0
problem is	0.08333333333333333
could make	0.25
Mary	0.00043830813061582295
-LRB- one	0.02127659574468085
version	0.00021915406530791147
her	0.00021915406530791147
unnecessary -RRB-	1.0
algorithms used	0.1111111111111111
introduced	0.00021915406530791147
follows	0.00021915406530791147
collocation .	0.5
the same	0.020942408376963352
computational lexicon	0.125
These rely	0.1
agreed	0.00021915406530791147
cataphoras in	1.0
Senseval -LRB-	0.2
To properly	0.3333333333333333
easy to	1.0
artificial	0.00021915406530791147
for some	0.02631578947368421
eschew	0.00021915406530791147
-LCB- car	1.0
weights for	1.0
constraints to	1.0
with the	0.16666666666666666
Senseval-2 ,	0.3333333333333333
to text	0.010101010101010102
to date	0.020202020202020204
and controversial	0.011764705882352941
reverse is	1.0
the Senseval	0.005235602094240838
substitutes	0.00021915406530791147
Knowledge sources	0.3333333333333333
combine	0.00021915406530791147
most popular	0.1
optimisation	0.00021915406530791147
examples ,	0.2
be required	0.03125
sense above	0.020833333333333332
data to	0.1111111111111111
and sentences	0.011764705882352941
Support	0.00021915406530791147
learning have	0.08333333333333333
evaluation	0.0026298487836949377
barely formulated	1.0
preserves the	1.0
to avoid	0.010101010101010102
variants :	0.5
clusters	0.00021915406530791147
trying such	1.0
, public	0.004291845493562232
senses induced	0.02857142857142857
properly	0.00021915406530791147
including measures	0.3333333333333333
coarse-grained than	0.2
did	0.00021915406530791147
In the	0.14285714285714285
are used	0.08823529411764706
evaluation is	0.08333333333333333
be it	0.03125
word senses	0.15517241379310345
baseline accuracy	1.0
deep	0.0006574621959237344
sense is	0.020833333333333332
-RCB-	0.00021915406530791147
glossaries ,	1.0
potentially ,	0.5
list of	1.0
cross-lingual	0.00021915406530791147
to compare	0.010101010101010102
and then	0.011764705882352941
sample had	0.16666666666666666
a tagging	0.010869565217391304
90	0.00021915406530791147
different algorithms	0.0625
the first	0.010471204188481676
other considerations	0.1111111111111111
to read	0.010101010101010102
rule-based	0.00021915406530791147
induction	0.0008766162612316459
automobile	0.00021915406530791147
Comparing and	1.0
broad distinctions	1.0
impediment	0.00021915406530791147
lectures ,	1.0
type of	1.0
concept of	0.5
information automatically	0.125
' selections	0.08333333333333333
extended	0.00043830813061582295
classifier being	0.25
upper bound	1.0
encoded	0.00043830813061582295
verb	0.00021915406530791147
statistically	0.00021915406530791147
testing	0.0008766162612316459
dictionary definitions	0.14285714285714285
and even	0.011764705882352941
bewildering variety	1.0
context of	0.16666666666666666
years	0.00021915406530791147
' properties	0.08333333333333333
External knowledge	1.0
-LRB- fleuve	0.02127659574468085
coverage of	1.0
in lexical	0.011363636363636364
senses ''	0.02857142857142857
semantic interpretation	0.25
levels of	1.0
mothers	0.00021915406530791147
is too	0.012987012987012988
classifications	0.00021915406530791147
task during	0.07692307692307693
, accuracy	0.004291845493562232
preferences	0.00021915406530791147
had strong	0.3333333333333333
combine supervised	1.0
a computer	0.010869565217391304
away	0.00021915406530791147
the Oxford	0.005235602094240838
context	0.0013149243918474688
knowledge such	0.043478260869565216
most researchers	0.1
access	0.00021915406530791147
having their	0.5
by Margaret	0.058823529411764705
all-words task	0.5
list	0.00021915406530791147
approaches ,	0.10526315789473684
improving relevance	1.0
, supervised	0.008583690987124463
and algorithms	0.011764705882352941
integrate	0.00021915406530791147
article	0.00043830813061582295
and to	0.011764705882352941
lexical databases	0.07692307692307693
some previously	0.09090909090909091
not agree	0.045454545454545456
may be	0.6666666666666666
as deep	0.02564102564102564
intelligence ,	1.0
result clustering	0.3333333333333333
best .	0.5
give	0.0006574621959237344
bank could	0.5
are	0.00745123822046899
days of	1.0
the process	0.005235602094240838
of knowledge	0.012903225806451613
which used	0.045454545454545456
from the	0.21428571428571427
relatedness and	1.0
frequency and	0.2
distinct word	0.25
Support Vector	1.0
different lectures	0.0625
' -LRB-	0.08333333333333333
tagging and	0.14285714285714285
substitution ,	0.5
Unsupervised	0.0008766162612316459
and procedures	0.011764705882352941
acquire	0.00021915406530791147
traditionally understood	1.0
top	0.00021915406530791147
sense	0.01051939513477975
thereby inducing	1.0
or rivière	0.03571428571428571
level -LRB-	0.5
relatively easy	1.0
of Sense-Tagged	0.0064516129032258064
song nearby	0.5
which to	0.045454545454545456
not at	0.045454545454545456
whether	0.00043830813061582295
</s> To	0.017543859649122806
: Identification	0.038461538461538464
practice ,	1.0
words like	0.027777777777777776
extracted	0.00021915406530791147
difﬁcult ,	1.0
intermediate language	1.0
word can	0.05172413793103448
in several	0.011363636363636364
in a	0.10227272727272728
choose samples	0.5
target words	0.3333333333333333
superior	0.00043830813061582295
anaphoras or	1.0
external	0.00021915406530791147
opposite is	1.0
In this	0.07142857142857142
the lack	0.020942408376963352
can go	0.038461538461538464
device -RRB-	1.0
the concept	0.005235602094240838
date ,	0.5
Bar-Hillel	0.00021915406530791147
``	0.0039447731755424065
computational	0.0017532325224632918
been applied	0.1
involve	0.00021915406530791147
becomes	0.00021915406530791147
the field	0.010471204188481676
state-of-the-art WSD	1.0
such a	0.09523809523809523
a word-aligned	0.010869565217391304
diversification	0.00021915406530791147
challenge for	1.0
biased .	1.0
hand-annotating	0.00021915406530791147
human	0.0021915406530791147
The former	0.041666666666666664
of untagged	0.0064516129032258064
decoupled	0.00021915406530791147
present much	1.0
</s> --	0.011695906432748537
their results	0.125
one meaning	0.06666666666666667
above 90	0.2
to different	0.010101010101010102
at	0.0019723865877712033
there are	0.3333333333333333
, stoplists	0.004291845493562232
sentence is	0.3333333333333333
-LRB- e.g.	0.0851063829787234
-LRB- polysemy	0.02127659574468085
applications ,	0.3333333333333333
is to	0.05194805194805195
often translated	0.16666666666666666
Yarowsky algorithm	0.5
frequent sense	1.0
supplements	0.00021915406530791147
'' are	0.06666666666666667
fraction of	1.0
2007	0.00021915406530791147
case of	0.5
retrieved	0.00021915406530791147
tasks should	0.2
reasoning	0.00021915406530791147
organization of	1.0
different test	0.0625
Some	0.00021915406530791147
of ways	0.0064516129032258064
1980s	0.00021915406530791147
This approach	0.25
task has	0.07692307692307693
source language	0.5
systems is	0.0625
a	0.020162174008327854
like words	0.2
small-scale	0.00021915406530791147
of `	0.01935483870967742
tag senses	1.0
sense Some	0.020833333333333332
enabling them	1.0
French -LRB-	0.5
comparisons	0.00021915406530791147
computational applications	0.125
algorithms to	0.2222222222222222
algorithm is	0.1111111111111111
all these	0.09090909090909091
hoped	0.00021915406530791147
used fine-grained	0.05
.	0.03747534516765286
while in	0.3333333333333333
of system	0.0064516129032258064
algorithms '	0.1111111111111111
words .	0.1111111111111111
musical instrument	1.0
ambiguous	0.00021915406530791147
the surrounding	0.005235602094240838
in senses	0.011363636363636364
WSD -RRB-	0.03125
extract a	1.0
standard or	0.5
and difficulty	0.011764705882352941
fleuve `	1.0
gives	0.00021915406530791147
In recent	0.07142857142857142
at all	0.1111111111111111
used :	0.05
animal	0.00021915406530791147
bottleneck is	0.25
bootstrapping	0.00043830813061582295
second sentence	0.3333333333333333
starts	0.00021915406530791147
bad	0.00021915406530791147
the task	0.010471204188481676
general to	0.5
These	0.0021915406530791147
Another problem	1.0
used on	0.05
HECTOR	0.00043830813061582295
Choices Sense	1.0
substitution	0.00043830813061582295
adjacent	0.00021915406530791147
deemed a	0.5
applications .	0.3333333333333333
recall are	1.0
, cluster-based	0.004291845493562232
edge of	1.0
used for	0.1
because such	0.125
and often	0.011764705882352941
</s> -LRB-	0.005847953216374269
, gives	0.004291845493562232
above example	0.2
is slippery	0.012987012987012988
cone ''	1.0
, etc.	0.012875536480686695
corpus .	0.06666666666666667
ambiguities	0.00021915406530791147
text ,	0.2222222222222222
classifications are	1.0
language is	0.1111111111111111
</s> Unsupervised	0.017543859649122806
not as	0.045454545454545456
' in	0.08333333333333333
sea bass	0.3333333333333333
spend	0.00021915406530791147
determine in	1.0
rate for	1.0
experimenters	0.00021915406530791147
are obviously	0.029411764705882353
belongs in	1.0
feature	0.00043830813061582295
consumed ,	1.0
e.g. the	0.25
1950s	0.00021915406530791147
nearby ,	1.0
word -LRB-	0.034482758620689655
translated into	1.0
own division	0.5
for other	0.02631578947368421
former one	0.25
parts of	0.8
gloss	0.00021915406530791147
whether the	0.5
word-sense relatedness	0.5
using broad	0.08333333333333333
be much	0.03125
each potentially	0.08333333333333333
variable	0.00021915406530791147
independently a	1.0
senses -LRB-	0.02857142857142857
conventional approaches	0.5
translation in	0.25
yet	0.00021915406530791147
bilingual corpora	0.5
Cross-Lingual	0.00021915406530791147
provide enough	0.25
e.g.	0.0008766162612316459
used	0.0043830813061582295
senses which	0.02857142857142857
given lexical	0.16666666666666666
significance and	1.0
support	0.00021915406530791147
number	0.00043830813061582295
evaluations	0.00021915406530791147
what	0.00043830813061582295
been put	0.05
their methods	0.125
model has	0.5
a metaphorical	0.010869565217391304
algorithms named	0.1111111111111111
of entropy	0.0064516129032258064
fine-grained WSD	0.2
labels	0.00021915406530791147
for ex	0.02631578947368421
to a	0.050505050505050504
sense the	0.020833333333333332
one must	0.06666666666666667
Moreover ,	1.0
finding the	1.0
cluster-based	0.00021915406530791147
then precision	0.2
some sea	0.09090909090909091
within	0.00043830813061582295
</s> Common	0.005847953216374269
adaptation	0.00043830813061582295
subject to	1.0
submitted for	1.0
all word	0.09090909090909091
trends	0.00021915406530791147
major	0.00021915406530791147
was an	0.058823529411764705
was decided	0.058823529411764705
Deep	0.00021915406530791147
and	0.018628095551172473
sense ontology	0.020833333333333332
-LRB- SemEval-2007	0.02127659574468085
another	0.00021915406530791147
dictionary -RRB-	0.14285714285714285
analyzing	0.00021915406530791147
It does	0.14285714285714285
mouse	0.00021915406530791147
Lesk algorithm	1.0
than once	0.1111111111111111
-LRB- they	0.02127659574468085
memory-based	0.00021915406530791147
possible	0.0006574621959237344
annotate all	1.0
overlapping	0.00021915406530791147
the common	0.005235602094240838
knowledge bases	0.043478260869565216
task --	0.07692307692307693
solution some	0.3333333333333333
adopting the	0.5
senses ;	0.02857142857142857
methods can	0.06666666666666667
became available	0.5
encoded in	0.5
it has	0.11764705882352941
speech are	0.16666666666666666
sisters	0.00043830813061582295
as writing	0.02564102564102564
In fact	0.07142857142857142
most systems	0.1
former was	0.25
part-of-speech	0.0006574621959237344
was used	0.058823529411764705
ensemble learning	1.0
, `	0.004291845493562232
tagged training	0.3333333333333333
, glossaries	0.004291845493562232
derived	0.00021915406530791147
much higher	0.16666666666666666
substitute	0.00021915406530791147
to definitions	0.010101010101010102
properly identify	1.0
comparative	0.00021915406530791147
disambiguation purposes	0.1
and overlapping	0.011764705882352941
example of	0.25
WSD for	0.03125
needed -RRB-	0.6666666666666666
evaluating different	1.0
assumption is	0.5
`` if	0.05555555555555555
exploited	0.00021915406530791147
generalize	0.00021915406530791147
early	0.0006574621959237344
state	0.00043830813061582295
because it	0.125
accuracy on	0.125
whether these	0.5
iterations is	1.0
these two	0.1111111111111111
computer-readable format	1.0
, gloss	0.004291845493562232
English-French machine	1.0
optimization ,	1.0
WordNet .	0.1111111111111111
slippery	0.00021915406530791147
by other	0.058823529411764705
Senseval-1 was	0.5
that preserves	0.03333333333333333
low	0.0006574621959237344
expensive to	1.0
systems and	0.0625
</s> Dictionary	0.005847953216374269
of ``	0.0064516129032258064
sense discreteness	0.020833333333333332
in similar	0.011363636363636364
can provide	0.038461538461538464
applicable in	1.0
seminal	0.00021915406530791147
the high-dimensionality	0.005235602094240838
words tend	0.027777777777777776
senses based	0.02857142857142857
closely related	1.0
can memorize	0.038461538461538464
retrieved document	1.0
computer can	0.2
give better	0.3333333333333333
block	0.00021915406530791147
is performed	0.012987012987012988
untagged corpora	0.5
both tree	0.3333333333333333
given	0.0013149243918474688
the computer	0.015706806282722512
ensemble	0.00021915406530791147
unfair	0.00021915406530791147
information retrieval	0.375
reasoning are	1.0
still knowledge-based	0.5
sub-meanings .	1.0
the WSD	0.005235602094240838
statistically analyzing	1.0
Word-aligned bilingual	1.0
into a	0.1
produce because	1.0
I went	0.5
some form	0.09090909090909091
thus overcoming	0.5
pine cone	1.0
For English	0.14285714285714285
specify the	1.0
cone	0.00021915406530791147
also gained	0.25
successively larger	1.0
terms of	1.0
Senseval workshop	0.4
art	0.0006574621959237344
coherence ,	1.0
with their	0.05555555555555555
not desired	0.045454545454545456
of another	0.0064516129032258064
Web search	0.5
fish ,	0.2
summary	0.00021915406530791147
been shown	0.2
analyzing those	1.0
assign	0.00021915406530791147
kernel-based	0.00021915406530791147
promising trends	1.0
-LRB- especially	0.02127659574468085
bootstrapping process	0.5
of popular	0.0064516129032258064
of word	0.03870967741935484
analysis	0.00021915406530791147
through	0.00021915406530791147
laborious	0.00043830813061582295
by increasing	0.058823529411764705
identify senses	1.0
reported	0.00043830813061582295
inference	0.00021915406530791147
coincide	0.00021915406530791147
every	0.0010957703265395574
Because of	1.0
and 57	0.011764705882352941
new knowledge	0.2
trees	0.00021915406530791147
train from	0.3333333333333333
revolution	0.00021915406530791147
In	0.0030681569143107606
-RRB- Ontologies	0.02127659574468085
</s> Thus	0.005847953216374269
was devised	0.058823529411764705
to provide	0.010101010101010102
purity -RRB-	1.0
task .	0.15384615384615385
of trying	0.0064516129032258064
are essential	0.029411764705882353
or selectional	0.03571428571428571
task	0.002849002849002849
writing instrument	0.5
supervised machine	0.14285714285714285
song is	0.5
under	0.00021915406530791147
that evoke	0.03333333333333333
and knowledge	0.011764705882352941
a variety	0.010869565217391304
long tradition	1.0
known	0.0008766162612316459
into lexical-sample	0.1
words tagged	0.027777777777777776
task consists	0.07692307692307693
main approaches	0.5
as is	0.02564102564102564
testing of	0.25
thesaurus	0.00021915406530791147
sentence	0.0006574621959237344
correctly	0.00021915406530791147
go fishing	0.5
second language	0.3333333333333333
high-dimensionality	0.00021915406530791147
-LRB- each	0.02127659574468085
'' ,	0.26666666666666666
French banque	0.5
MRDs -RRB-	1.0
speech to	0.16666666666666666
</s> WSD	0.023391812865497075
evidence on	0.5
for every	0.05263157894736842
sense-annotated examples	0.3333333333333333
words of	0.027777777777777776
pair	0.00043830813061582295
probably because	0.3333333333333333
is obvious	0.012987012987012988
sample consists	0.16666666666666666
on	0.006136313828621521
works ,	1.0
linguistic	0.00021915406530791147
thereby	0.00021915406530791147
encodes concepts	1.0
ontology .	1.0
field	0.00043830813061582295
to each	0.010101010101010102
organize	0.00021915406530791147
languages for	0.3333333333333333
meaning into	0.125
applied	0.00043830813061582295
from those	0.07142857142857142
does not	1.0
</s> History	0.005847953216374269
each	0.0026298487836949377
car	0.00043830813061582295
upper	0.00021915406530791147
of iterations	0.0064516129032258064
homograph level	0.5
primarily	0.00043830813061582295
supervised methods	0.21428571428571427
is in	0.025974025974025976
classifier	0.0008766162612316459
bass line	0.08333333333333333
, respectively	0.004291845493562232
laborious testing	0.5
outperforming them	1.0
scientists	0.00021915406530791147
available .	0.5
exist ,	0.3333333333333333
also known	0.25
help in	1.0
of dictionary	0.012903225806451613
To a	0.3333333333333333
indeed	0.00021915406530791147
than those	0.1111111111111111
full-fledged	0.00021915406530791147
alternative	0.00021915406530791147
tagging judgement	0.14285714285714285
shown superior	0.2
-RRB- words	0.02127659574468085
around	0.00043830813061582295
word to	0.017241379310344827
are sisters	0.058823529411764705
unfair or	1.0
people	0.00043830813061582295
formulated	0.00043830813061582295
processing	0.0006574621959237344
controversial	0.00021915406530791147
two examples	0.2
test one	0.16666666666666666
's	0.0015340784571553803
`` senses	0.05555555555555555
latter is	0.25
incline to	1.0
cluster occurrences	1.0
main	0.00043830813061582295
the original	0.005235602094240838
the evaluation	0.005235602094240838
senses of	0.02857142857142857
takes the	1.0
reference sense	1.0
disambiguating all	0.2
still	0.00043830813061582295
England ,	1.0
went fishing	1.0
disambiguate are	0.2
engines ,	0.5
of common	0.0064516129032258064
these same	0.1111111111111111
already publicly	1.0
successful approaches	0.25
of artificial	0.0064516129032258064
amount	0.0006574621959237344
polysemy	0.0006574621959237344
raw	0.00043830813061582295
Still ,	1.0
hand-coded they	1.0
task-dependency A	1.0
identifying	0.00021915406530791147
, known	0.004291845493562232
meanings -LRB-	0.25
especially Yarowsky	0.5
corpus evidence	0.06666666666666667
been organized	0.05
Web	0.0008766162612316459
block of	1.0
proposed as	1.0
seminal dictionary-based	1.0
correspond to	1.0
better	0.0010957703265395574
requires a	0.5
knowledge does	0.043478260869565216
translation	0.0008766162612316459
strong relationships	1.0
limited	0.00043830813061582295
13	0.00021915406530791147
per collocation	0.5
all the	0.18181818181818182
Finally ,	1.0
result lists	0.3333333333333333
word-aligned bilingual	1.0
bass	0.0026298487836949377
inventory and	0.09090909090909091
dictionary-based .	0.3333333333333333
</s> Generally	0.005847953216374269
most	0.0021915406530791147
reached .	1.0
hand --	1.0
between dictionaries	0.5
replaced with	1.0
sample ''	0.16666666666666666
The seeds	0.041666666666666664
every three	0.2
to annotate	0.010101010101010102
low frequency	1.0
</s> Moreover	0.011695906432748537
is deciding	0.012987012987012988
supervised models	0.07142857142857142
shown that	0.2
since they	0.16666666666666666
selectional	0.00043830813061582295
then used	0.4
</s> The	0.12280701754385964
facts .	1.0
the definitions	0.020942408376963352
had to	0.3333333333333333
often small-scale	0.16666666666666666
the simplest	0.005235602094240838
a requisite	0.010869565217391304
Inter-judge variance	1.0
a mapping	0.010869565217391304
for	0.008327854481700635
computer	0.0010957703265395574
is perhaps	0.012987012987012988
automatically	0.0008766162612316459
revolution swept	1.0
% ,	0.2222222222222222
</s> These	0.03508771929824561
corpus is	0.2
of Lesk	0.0064516129032258064
a retrieved	0.010869565217391304
given the	0.3333333333333333
looked	0.00021915406530791147
WSD	0.007012930089853167
corpora loose	0.1
one	0.003287310979618672
1970s ,	1.0
Developing	0.00021915406530791147
expensive	0.00043830813061582295
the point	0.005235602094240838
, developers	0.004291845493562232
than that	0.1111111111111111
be induced	0.03125
most confident	0.1
Sense inventory	0.6666666666666666
as it	0.02564102564102564
data for	0.1111111111111111
up easily	1.0
HECTOR sense	0.5
most frequent	0.2
of a	0.06451612903225806
In cases	0.07142857142857142
cope with	1.0
kinds of	1.0
accuracy in	0.125
Knowledge ,	0.3333333333333333
5 indeterminates	1.0
These eschew	0.1
and may	0.011764705882352941
comprises disambiguating	1.0
or song	0.03571428571428571
bass -LRB-	0.25
an initial	0.09090909090909091
machine translation	0.375
the former	0.015706806282722512
some indeterminants	0.09090909090909091
set ,	0.16666666666666666
early example	0.3333333333333333
of low	0.0064516129032258064
its	0.0010957703265395574
Comparison of	1.0
reverse	0.00021915406530791147
organized	0.00021915406530791147
figures	0.00021915406530791147
clusters\/senses	0.00021915406530791147
case	0.00043830813061582295
semi-supervised and	0.25
sense-annotated	0.0006574621959237344
always indicates	0.3333333333333333
corpora for	0.1
, word-sense	0.004291845493562232
compare	0.00021915406530791147
other	0.0019723865877712033
graph-based	0.00043830813061582295
by sentences	0.058823529411764705
infinitely variable	1.0
large quantities	1.0
-LRB- IR	0.02127659574468085
relatively	0.00021915406530791147
tagging are	0.14285714285714285
Current accuracy	0.5
to help	0.010101010101010102
acquisition	0.0008766162612316459
2000s	0.00021915406530791147
in information	0.011363636363636364
work by	0.2
is ,	0.012987012987012988
occur in	1.0
the largest	0.005235602094240838
these supervised	0.1111111111111111
which sense	0.13636363636363635
, inference	0.004291845493562232
could improve	0.25
deciding	0.00021915406530791147
a river	0.010869565217391304
better performance	0.2
procedures	0.00021915406530791147
during Senseval-1	0.5
were classified	0.16666666666666666
knowing	0.00021915406530791147
mapping	0.00021915406530791147
their time	0.125
World	0.00021915406530791147
open	0.00021915406530791147
the tagged	0.005235602094240838
related with	0.3333333333333333
a choice	0.010869565217391304
similar senses	0.5
bases	0.00021915406530791147
sense .	0.041666666666666664
, because	0.008583690987124463
algorithm of	0.1111111111111111
first attempt	0.2
one 's	0.06666666666666667
exemplified	0.00021915406530791147
-LRB- in	0.0425531914893617
other cases	0.1111111111111111
datasets	0.00021915406530791147
make use	0.5
indeterminates -RRB-	1.0
In order	0.14285714285714285
finer-grained	0.00021915406530791147
later the	0.5
the appropriate	0.005235602094240838
normally	0.00043830813061582295
the state	0.005235602094240838
present	0.00021915406530791147
selectional preferences	0.5
to tag	0.010101010101010102
that supplements	0.03333333333333333
one can	0.13333333333333333
solving	0.00021915406530791147
algorithm or	0.1111111111111111
Naïve	0.00021915406530791147
sampling	0.00021915406530791147
ambiguity of	1.0
disagreements	0.00021915406530791147
be automatically	0.03125
Unsupervised learning	0.25
choose	0.00043830813061582295
rely on	0.6666666666666666
apply	0.00021915406530791147
citation	0.00043830813061582295
seeds are	1.0
appropriate	0.00021915406530791147
be the	0.03125
of accuracy	0.0064516129032258064
approach ,	0.5
of search	0.0064516129032258064
approaches in	0.05263157894736842
mainly	0.0006574621959237344
via	0.00021915406530791147
2004 -RRB-	1.0
replicate this	1.0
it	0.003725619110234495
difficulty	0.00021915406530791147
and ensemble	0.011764705882352941
Current English	0.5
can disambiguate	0.038461538461538464
Ontologies Unstructured	1.0
, automatically	0.004291845493562232
obvious that	1.0
described	0.00021915406530791147
presume	0.00021915406530791147
types of	0.5
from raw	0.07142857142857142
individuals	0.00021915406530791147
clustering by	0.5
on particular	0.03571428571428571
</s> Senseval	0.005847953216374269
techniques use	0.1111111111111111
word :	0.017241379310344827
Initially only	1.0
those of	0.25
serves as	1.0
around each	0.5
coarse-grained distinctions	0.2
Design Choices	1.0
into senses	0.3
They can	0.6666666666666666
an open	0.09090909090909091
to train	0.030303030303030304
words in	0.1111111111111111
word .	0.034482758620689655
available :	0.5
is encoded	0.012987012987012988
variety	0.0006574621959237344
word-sense	0.00043830813061582295
They just	0.3333333333333333
one sense	0.06666666666666667
induction or	0.25
Mary are	1.0
research results	0.125
host of	1.0
algorithm was	0.1111111111111111
evaluation datasets	0.08333333333333333
state without	0.5
known dictionary	0.25
frequency	0.0010957703265395574
Most	0.00043830813061582295
probably in	0.3333333333333333
river	0.0006574621959237344
pronouns	0.00021915406530791147
discussion	0.00021915406530791147
users	0.00021915406530791147
can not	0.07692307692307693
in ``	0.022727272727272728
-LRB- instrument	0.02127659574468085
the Web	0.005235602094240838
'' and	0.2
successful	0.0008766162612316459
where the	0.5
On finer-grained	1.0
point where	1.0
dictionary senses	0.2857142857142857
with words	0.1111111111111111
, however	0.017167381974248927
the statistical	0.005235602094240838
sufficiently	0.00043830813061582295
this works	0.2
of total	0.0064516129032258064
manually	0.0008766162612316459
, human	0.004291845493562232
the tree	0.005235602094240838
desired	0.00021915406530791147
particular dictionary	0.5
definitions of	0.3333333333333333
level	0.0008766162612316459
</s> It	0.04093567251461988
</s> Semi-supervised	0.011695906432748537
homographs	0.00021915406530791147
all-words	0.00043830813061582295
public evaluation	1.0
. ''	1.0
avoid the	1.0
an international	0.09090909090909091
further	0.00043830813061582295
better than	0.4
, became	0.004291845493562232
and techniques	0.011764705882352941
a substitute	0.010869565217391304
even outperforming	0.3333333333333333
then	0.0010957703265395574
cases division	0.25
senses .	0.17142857142857143
''	0.0039447731755424065
To give	0.3333333333333333
providing a	1.0
to later	0.010101010101010102
have	0.003287310979618672
the immediately	0.005235602094240838
's not	0.14285714285714285
using the	0.16666666666666666
improve	0.00021915406530791147
while theoretically	0.3333333333333333
eligible	0.00021915406530791147
Early	0.00021915406530791147
WSD using	0.03125
submitted	0.00021915406530791147
that similar	0.03333333333333333
word in	0.05172413793103448
accurate	0.00021915406530791147
dominant word	1.0
by finding	0.058823529411764705
steadily to	1.0
be observed	0.03125
narrow ones	1.0
reminiscent of	1.0
relevance	0.00021915406530791147
It was	0.14285714285714285
went	0.00021915406530791147
WSD was	0.0625
were largely	0.16666666666666666
approaches normally	0.05263157894736842
define	0.00021915406530791147
, knowing	0.004291845493562232
sense in	0.041666666666666664
fish sense	0.2
Masterman	0.00021915406530791147
, these	0.004291845493562232
problem .	0.16666666666666666
assumed	0.00021915406530791147
from corpora	0.07142857142857142
within an	0.5
the other	0.010471204188481676
the art	0.005235602094240838
indeterminates	0.00021915406530791147
of sense	0.0064516129032258064
devised in	1.0
test in	0.16666666666666666
divide	0.00021915406530791147
but not	0.18181818181818182
a larger	0.010869565217391304
in terms	0.011363636363636364
-RRB- used	0.02127659574468085
competitions parts	0.5
has two	0.0625
corpus as	0.06666666666666667
</s> Human	0.005847953216374269
knowledge-based	0.0013149243918474688
the sense	0.015706806282722512
groups	0.00021915406530791147
work directly	0.2
about word	1.0
full-fledged coarse-grained	1.0
, auto	0.004291845493562232
appearing	0.00021915406530791147
unimportant .	1.0
repetitions in	1.0
WordNet is	0.2222222222222222
corpus for	0.06666666666666667
presume access	1.0
the word	0.05759162303664921
reach a	1.0
problem of	0.16666666666666666
line	0.00021915406530791147
make	0.0008766162612316459
the untagged	0.005235602094240838
cluster	0.00021915406530791147
research have	0.125
order	0.00043830813061582295
more	0.0017532325224632918
sentence ,	0.6666666666666666
text to	0.1111111111111111
to their	0.010101010101010102
words one	0.027777777777777776
the possible	0.005235602094240838
% accuracy	0.2222222222222222
incline	0.00021915406530791147
shifted to	1.0
referred	0.00021915406530791147
, improving	0.004291845493562232
in some	0.011363636363636364
interpretation systems	1.0
</s> An	0.005847953216374269
language	0.0019723865877712033
AI research	0.5
While	0.00021915406530791147
dogs	0.00043830813061582295
latter all	0.25
crucially	0.00021915406530791147
semantically .	1.0
had	0.0006574621959237344
avoid	0.00021915406530791147
of context	0.0064516129032258064
system assignments	0.25
Knowledge	0.0006574621959237344
depending	0.00021915406530791147
almost always	0.3333333333333333
Douglas Lenat	1.0
polysemy level	0.3333333333333333
</s> Both	0.005847953216374269
</s> More	0.005847953216374269
sets .	0.3333333333333333
under the	1.0
to choose	0.020202020202020204
distinctions in	0.2222222222222222
natural language	1.0
coverage	0.00021915406530791147
or conventional	0.03571428571428571
the ``	0.005235602094240838
Unit	0.00021915406530791147
Shallow	0.00021915406530791147
Still	0.00021915406530791147
to compute	0.010101010101010102
comprises	0.00021915406530791147
is a	0.03896103896103896
or fishing	0.03571428571428571
homograph -RRB-	0.5
punched-card version	1.0
, automobile	0.004291845493562232
solving the	1.0
preferences -LRB-	1.0
learning is	0.08333333333333333
rules can	0.5
a particular	0.010869565217391304
Word meaning	0.5
relationships	0.00021915406530791147
provide data	0.25
cooks food	1.0
use	0.0019723865877712033
, mainly	0.008583690987124463
sense ''	0.020833333333333332
This	0.0008766162612316459
cetera	0.00021915406530791147
for repetitions	0.02631578947368421
This classifier	0.25
on such	0.03571428571428571
-LRB- animal	0.02127659574468085
it was	0.058823529411764705
, thesauri	0.008583690987124463
these	0.0019723865877712033
of Current	0.0064516129032258064
The bass	0.041666666666666664
named	0.00043830813061582295
: These	0.15384615384615385
modulated ,	1.0
running	0.00021915406530791147
typically cooks	1.0
or enclosure	0.03571428571428571
measures of	0.25
years since	1.0
Task Design	1.0
high-dimensionality of	1.0
accuracies	0.00021915406530791147
:	0.005698005698005698
Learner	0.00021915406530791147
best	0.00043830813061582295
consists in	0.6666666666666666
realistic evaluation	0.5
And comparing	0.5
resolved	0.00021915406530791147
same meaning	0.2
approaches presume	0.05263157894736842
95	0.00021915406530791147
an upper	0.09090909090909091
a more	0.010869565217391304
assigned by	1.0
known under	0.25
Recall	0.00021915406530791147
applied with	0.5
separately -LRB-	1.0
the corpus	0.020942408376963352
technology	0.00021915406530791147
can agree	0.038461538461538464
confident classifications	1.0
different variants	0.0625
one of	0.13333333333333333
sense facts	0.020833333333333332
access to	1.0
on manual	0.03571428571428571
exemplified by	1.0
them in	0.3333333333333333
</s> And	0.011695906432748537
ability can	1.0
several kinds	1.0
decisions	0.00021915406530791147
The first	0.041666666666666664
a set	0.03260869565217391
further exemplified	0.5
research ,	0.125
specific	0.00021915406530791147
techniques have	0.2222222222222222
methods -LRB-	0.03333333333333333
impediment to	1.0
held	0.00021915406530791147
as powerful	0.02564102564102564
The difference	0.041666666666666664
reported that	0.5
-RRB- can	0.02127659574468085
new occurrences	0.2
language depending	0.1111111111111111
tested by	0.3333333333333333
a human	0.03260869565217391
trying	0.00021915406530791147
word-aligned	0.00021915406530791147
lexicon that	0.5
can be	0.5
and her	0.011764705882352941
learning algorithm	0.08333333333333333
research on	0.125
crucially on	1.0
base .	0.5
: each	0.038461538461538464
Senseval exercises	0.2
weak	0.00021915406530791147
parse	0.00021915406530791147
the amount	0.005235602094240838
substitute for	1.0
assigned	0.00021915406530791147
without a	0.3333333333333333
semi-supervised learning	0.25
deep approaches	1.0
memorize all	1.0
devised	0.00021915406530791147
work well	0.2
its numbered	0.2
always agree	0.3333333333333333
available	0.00043830813061582295
almost	0.0006574621959237344
is true	0.012987012987012988
resources used	0.14285714285714285
confused by	1.0
difference consists	1.0
sense was	0.020833333333333332
process ,	0.3333333333333333
have to	0.06666666666666667
different words	0.0625
large	0.00021915406530791147
financial bank	1.0
Inventories .	1.0
knowledge-based methods	0.6666666666666666
An	0.00021915406530791147
learning methods	0.08333333333333333
also useful	0.25
the research	0.005235602094240838
task requires	0.07692307692307693
Two shallow	0.5
domain adaptation	0.5
are to	0.029411764705882353
ambiguity	0.00021915406530791147
contexts ,	1.0
Thus	0.00021915406530791147
word occurrences	0.05172413793103448
they are	0.3333333333333333
replicate	0.00021915406530791147
workshop the	1.0
related	0.0006574621959237344
sub-meanings	0.00021915406530791147
overcoming discreteness	1.0
, in	0.017167381974248927
base such	0.5
to take	0.010101010101010102
'' is	0.13333333333333333
to WordNet	0.010101010101010102
spreading	0.00021915406530791147
international word	1.0
previously chosen	0.3333333333333333
the HECTOR	0.010471204188481676
hint how	1.0
Graph-based	0.00043830813061582295
significant meaning	1.0
to	0.021696252465483234
a dictionary	0.010869565217391304
dictionary ,	0.2857142857142857
enough evidence	0.5
to exhibit	0.010101010101010102
completely unsupervised	0.3333333333333333
SemEval	0.00043830813061582295
handful of	1.0
recent	0.0006574621959237344
; if	0.25
compared	0.0006574621959237344
senses	0.007670392285776901
was	0.003725619110234495
that	0.006574621959237344
they were	0.16666666666666666
instructive to	1.0
and thesauruses	0.011764705882352941
ways	0.00021915406530791147
gloss WSD	1.0
compare the	1.0
frequency lists	0.2
trained	0.00043830813061582295
Wikipedia	0.00043830813061582295
usually driven	0.5
is based	0.012987012987012988
always	0.0006574621959237344
forays into	1.0
n't try	1.0
word on	0.017241379310344827
a knowledge	0.010869565217391304
was 51.4	0.058823529411764705
spreading activation	1.0
first introduced	0.2
The task	0.041666666666666664
translated	0.00021915406530791147
discourse	0.0006574621959237344
the senses	0.010471204188481676
popular	0.00043830813061582295
interpretation	0.00021915406530791147
include the	0.3333333333333333
his	0.00021915406530791147
things	0.00021915406530791147
other .	0.1111111111111111
is reached	0.012987012987012988
all of	0.18181818181818182
river '	1.0
: a	0.038461538461538464
fine-grained distinctions	0.2
with senses	0.05555555555555555
</s> Deep	0.005847953216374269
and it	0.011764705882352941
problem on	0.08333333333333333
was already	0.058823529411764705
tree which	0.3333333333333333
Identification of	1.0
5	0.00021915406530791147
question whether	1.0
coherent	0.00021915406530791147
body	0.00043830813061582295
This model	0.25
</s> Almost	0.005847953216374269
approaches used	0.05263157894736842
lexical information	0.07692307692307693
per	0.00043830813061582295
to assign	0.010101010101010102
the 1950s	0.005235602094240838
state of	0.5
manually annotated	0.25
The process	0.041666666666666664
to generalize	0.010101010101010102
other ,	0.1111111111111111
allows	0.00021915406530791147
caveats	0.00021915406530791147
time	0.00043830813061582295
sample organizers	0.16666666666666666
set	0.0013149243918474688
words further	0.027777777777777776
In English	0.07142857142857142
variants	0.00043830813061582295
the latter	0.015706806282722512
sisters of	0.5
more -RRB-	0.125
since	0.0013149243918474688
in most	0.011363636363636364
, held	0.004291845493562232
Bayes	0.00021915406530791147
finer-grained sense	1.0
a new	0.010869565217391304
bottleneck because	0.25
a corpus	0.021739130434782608
not tend	0.045454545454545456
previously selected	0.3333333333333333
clearly	0.00021915406530791147
explain the	1.0
These include	0.1
by a	0.058823529411764705
e.g. in	0.25
whim of	1.0
Senseval-2 -RRB-	0.3333333333333333
disambiguation competition	0.1
the BNC	0.005235602094240838
such inventories	0.047619047619047616
the fish	0.005235602094240838
inducing	0.00021915406530791147
a bootstrapping	0.010869565217391304
constraints	0.00021915406530791147
adopting	0.00043830813061582295
by	0.003725619110234495
a full-fledged	0.010869565217391304
n content	0.5
just	0.00043830813061582295
into different	0.1
two	0.0010957703265395574
to as	0.010101010101010102
</s> Developing	0.005847953216374269
tested	0.0006574621959237344
approach starts	0.5
determined by	1.0
fact	0.00021915406530791147
on the	0.25
</s> Recently	0.011695906432748537
which the	0.045454545454545456
in case	0.011363636363636364
Vector Machines	1.0
assignments	0.00021915406530791147
From observation	1.0
evoke	0.00021915406530791147
unsupervised	0.0006574621959237344
required ,	0.5
derived by	1.0
necessarily required	1.0
2000s saw	1.0
small amount	0.25
process of	0.3333333333333333
sample of	0.16666666666666666
in lack	0.011363636363636364
An alternative	1.0
paradigm problem	1.0
the experiments	0.005235602094240838
the shallow	0.005235602094240838
and work	0.011764705882352941
any	0.0006574621959237344
processing ,	0.6666666666666666
community ,	1.0
Another	0.00021915406530791147
databases .	1.0
of very	0.0064516129032258064
of earlier	0.0064516129032258064
correct	0.00021915406530791147
the form	0.010471204188481676
of instances	0.0064516129032258064
general	0.00043830813061582295
one level	0.06666666666666667
concepts as	1.0
-- named	0.16666666666666666
achieving over	1.0
adopted	0.0006574621959237344
Differences between	1.0
during the	0.5
</s> Lexical	0.005847953216374269
engines	0.00043830813061582295
more accurate	0.125
disagreements arise	1.0
translate	0.00021915406530791147
meanings	0.0008766162612316459
cataphoras	0.00021915406530791147
verb tasks	1.0
to coincide	0.010101010101010102
campaigns	0.00043830813061582295
has shifted	0.0625
electronic computer	1.0
unlabeled or	0.5
the standard	0.005235602094240838
very notion	0.14285714285714285
Among	0.00021915406530791147
unanimously	0.00021915406530791147
just use	0.5
1950s .	1.0
maximum	0.00021915406530791147
the words	0.031413612565445025
to model	0.010101010101010102
reason for	1.0
continue to	1.0
Some AI	1.0
while it	0.3333333333333333
or metonymic	0.03571428571428571
resolution ,	1.0
unimportant	0.00021915406530791147
senses is	0.05714285714285714
field of	1.0
not very	0.09090909090909091
automobile ,	1.0
words for	0.027777777777777776
perform a	0.25
then disambiguate	0.2
to give	0.010101010101010102
The bootstrapping	0.041666666666666664
or discrimination	0.03571428571428571
infinitely	0.00021915406530791147
large-scale	0.00021915406530791147
a secondary	0.010869565217391304
is probably	0.012987012987012988
process repeats	0.3333333333333333
results on	0.25
connectivity measures	1.0
its successor	0.2
associated techniques	1.0
sounds as	0.5
knowledge in	0.043478260869565216
these ,	0.1111111111111111
including associated	0.3333333333333333
looked for	1.0
data .	0.1111111111111111
mining	0.00021915406530791147
also true	0.25
Senseval-3	0.00021915406530791147
trained for	0.5
thesauruses	0.00021915406530791147
to significant	0.010101010101010102
meaning -RRB-	0.125
each task	0.08333333333333333
true of	0.5
can take	0.07692307692307693
-LRB- such	0.0425531914893617
, completely	0.004291845493562232
that words	0.03333333333333333
definitions	0.0013149243918474688
Oxford Advanced	1.0
Choices	0.00021915406530791147
dictionaries	0.0013149243918474688
15	0.00021915406530791147
contexts	0.00021915406530791147
the feature	0.005235602094240838
Research has	0.5
problems in	1.0
, humans	0.004291845493562232
including all-words	0.3333333333333333
substantial	0.00021915406530791147
such as	0.6190476190476191
inter-judge variance	1.0
Discreteness	0.00021915406530791147
accuracy ,	0.125
if such	0.2
that WSD	0.03333333333333333
, and	0.09012875536480687
used as	0.1
fish	0.0010957703265395574
sometimes the	1.0
decision trees	0.5
distinctions are	0.1111111111111111
POS-tagging	0.00021915406530791147
England	0.00021915406530791147
Lenat	0.00021915406530791147
</s> There	0.005847953216374269
attempt was	0.5
baselines are	1.0
driven	0.00021915406530791147
some measure	0.09090909090909091
discrimination .	1.0
the source	0.005235602094240838
is often	0.025974025974025976
However	0.0017532325224632918
performance close	0.14285714285714285
intelligence	0.00021915406530791147
algorithm .	0.3333333333333333
organized .	1.0
, until	0.004291845493562232
anaphora resolution	1.0
far more	0.5
and WSD	0.011764705882352941
well .	0.3333333333333333
used in	0.15
performance on	0.14285714285714285
accuracy of	0.125
such approaches	0.047619047619047616
coherence	0.00021915406530791147
first formulated	0.2
extension of	1.0
multiple meanings	1.0
humans will	0.5
One problem	0.25
the text	0.020942408376963352
inclusion POS-tagging	1.0
implement	0.00021915406530791147
The systems	0.041666666666666664
word instances	0.017241379310344827
enclosure	0.00021915406530791147
real	0.00021915406530791147
Different	0.00021915406530791147
% and	0.1111111111111111
typical for	1.0
difﬁcult	0.00021915406530791147
barely	0.00021915406530791147
instrument or	0.2
famous	0.00021915406530791147
to less	0.010101010101010102
, world	0.004291845493562232
perform almost	0.25
judgement ,	1.0
rate	0.00021915406530791147
clear	0.00021915406530791147
annotated with	0.3333333333333333
' ,	0.16666666666666666
without using	0.3333333333333333
publicly available	1.0
provided as	1.0
lack	0.0010957703265395574
senses or	0.02857142857142857
methods Because	0.03333333333333333
, training	0.004291845493562232
These rules	0.1
developed within	1.0
deemed unnecessary	0.5
slippery and	1.0
writing	0.00043830813061582295
search result	0.3333333333333333
read	0.00021915406530791147
knowledge resources	0.043478260869565216
fundamental	0.00021915406530791147
restrictions -RRB-	1.0
memorize	0.00043830813061582295
scientists incline	1.0
rather than	1.0
promising	0.00021915406530791147
A rich	0.25
in which	0.056818181818181816
bass '	0.08333333333333333
may	0.0006574621959237344
kernel-based methods	1.0
they can	0.16666666666666666
unannotated	0.00021915406530791147
an intermediate	0.09090909090909091
</s> During	0.011695906432748537
written -RRB-	1.0
`` electronic	0.05555555555555555
Evidence .	1.0
the 1940s	0.005235602094240838
the degree	0.005235602094240838
Main	0.00043830813061582295
WSD problem	0.03125
known part	0.25
untagged portion	0.5
normally tested	0.5
discourse '	0.3333333333333333
</s> Lexicographers	0.005847953216374269
speech of	0.16666666666666666
bad performance	1.0
trees .	1.0
thesauruses will	1.0
where WSD	0.5
solved	0.00021915406530791147
take	0.0006574621959237344
common	0.0008766162612316459
achieving	0.00021915406530791147
lexicon of	0.5
machines	0.00021915406530791147
supervised learning	0.21428571428571427
WSD in	0.03125
</s> Shallow	0.005847953216374269
which often	0.045454545454545456
speech .	0.16666666666666666
rich lexical	0.5
motorcar	0.00021915406530791147
, above	0.004291845493562232
quality	0.00021915406530791147
popular example	0.5
-- deep	0.16666666666666666
workshop	0.00043830813061582295
or world	0.03571428571428571
own to	0.5
restrictions	0.00021915406530791147
for different	0.02631578947368421
of results	0.0064516129032258064
, at	0.008583690987124463
properties of	1.0
directly from	1.0
cluster-based evaluations	1.0
the ambiguity	0.005235602094240838
precision	0.00021915406530791147
disambiguation ;	0.1
are mothers	0.029411764705882353
English-French	0.00021915406530791147
target word	0.3333333333333333
fraction	0.00043830813061582295
WordNet as	0.1111111111111111
manual	0.00021915406530791147
not dependent	0.045454545454545456
such an	0.047619047619047616
researched	0.00021915406530791147
things separately	1.0
its own	0.4
useful	0.00021915406530791147
and unsupervised	0.011764705882352941
even	0.0006574621959237344
1980s large-scale	1.0
lexicography	0.00021915406530791147
preparing and	1.0
Structured :	1.0
been used	0.05
providing	0.00021915406530791147
amounts	0.00021915406530791147
disambiguate the	0.2
example	0.0017532325224632918
but	0.002410694718387026
ever	0.00021915406530791147
human .	0.1
usually integrate	0.5
-RRB-	0.010300241069471838
outside	0.00021915406530791147
is far	0.012987012987012988
point	0.00021915406530791147
The	0.005259697567389875
simple knowledge-based	0.25
any supervised	0.3333333333333333
linguistics .	0.25
word selection	0.017241379310344827
fish -RRB-	0.2
train	0.0006574621959237344
1940s	0.00021915406530791147
which requires	0.045454545454545456
very successful	0.2857142857142857
amount of	1.0
was mainly	0.058823529411764705
corpora and	0.1
simple and	0.25
to acquire	0.010101010101010102
state-of-the-art	0.00021915406530791147
obvious	0.00021915406530791147
the greatest	0.010471204188481676
any real	0.3333333333333333
languages ,	0.3333333333333333
1960	0.00021915406530791147
and knowledge-based	0.03529411764705882
discover in	1.0
of meaning	0.0064516129032258064
the major	0.005235602094240838
a hint	0.010869565217391304
From	0.00021915406530791147
-LRB- 2004	0.02127659574468085
speech	0.0013149243918474688
must know	0.5
role	0.00021915406530791147
these approaches	0.1111111111111111
discreteness	0.00043830813061582295
</s> Most	0.011695906432748537
as degree	0.02564102564102564
this case	0.2
knowledge	0.0050405435020819634
a punched-card	0.010869565217391304
like pronouns	0.2
obviously different	1.0
matter	0.00021915406530791147
laborious and	0.5
or	0.006136313828621521
inventory for	0.09090909090909091
1998 -RRB-	0.5
many	0.00021915406530791147
having anaphoras	0.5
in dictionaries	0.011363636363636364
Lexicographers	0.00021915406530791147
maximum number	1.0
document ;	1.0
attempt	0.00043830813061582295
different sense	0.0625
often	0.0013149243918474688
tagged	0.0006574621959237344
induced	0.0006574621959237344
be closely	0.03125
renamed SemEval	1.0
similar	0.00043830813061582295
pine	0.00021915406530791147
fundamental component	1.0
as synonym	0.02564102564102564
extremely difﬁcult	1.0
in word	0.011363636363636364
to completely	0.010101010101010102
World Wide	1.0
the competition	0.005235602094240838
For comparison	0.14285714285714285
WSD evaluation	0.0625
of topics	0.0064516129032258064
, probably	0.004291845493562232
computer 's	0.2
can	0.005698005698005698
for individuals	0.02631578947368421
model all	0.5
for example	0.02631578947368421
applications	0.0006574621959237344
or parts	0.03571428571428571
a second	0.010869565217391304
requires	0.00043830813061582295
using some	0.08333333333333333
instances correctly	0.5
are also	0.029411764705882353
frequently discover	1.0
sometimes	0.00021915406530791147
; Domain-driven	0.25
work	0.0010957703265395574
Warren	0.00021915406530791147
chosen	0.0006574621959237344
These make	0.2
` bass	0.1
potentially	0.00043830813061582295
been traditionally	0.05
be mapped	0.03125
disambiguate words	0.2
so the	0.25
</s> Sense	0.011695906432748537
has multiple	0.0625
than 75	0.1111111111111111
59.1 %	1.0
at hand	0.1111111111111111
but had	0.09090909090909091
Early researchers	1.0
of speech	0.03870967741935484
main performance	0.5
was first	0.058823529411764705
Performance has	1.0
indeed ,	1.0
Alternatively	0.00021915406530791147
observation ,	1.0
or most	0.03571428571428571
of testing	0.0064516129032258064
each other	0.16666666666666666
exercises -LRB-	0.25
disambiguation .	0.1
95 %	1.0
information to	0.125
comparing these	0.5
the sea	0.005235602094240838
being	0.0010957703265395574
tones	0.00021915406530791147
retrieval ,	0.3333333333333333
More	0.00021915406530791147
second Senseval	0.3333333333333333
broad	0.00021915406530791147
85 %	1.0
Word sense	0.5
recall	0.00021915406530791147
is deemed	0.012987012987012988
'' because	0.06666666666666667
Design	0.00021915406530791147
methods depend	0.03333333333333333
Roget	0.00043830813061582295
a fundamental	0.010869565217391304
coded knowledge	1.0
from a	0.07142857142857142
target	0.0013149243918474688
essential	0.00021915406530791147
and exploited	0.011764705882352941
which is	0.045454545454545456
understood the	0.5
disambiguation	0.0021915406530791147
rely	0.0006574621959237344
1960 -RRB-	1.0
argue	0.00043830813061582295
meaning is	0.125
Senseval-3 -LRB-	1.0
and sense-annotated	0.011764705882352941
measure	0.00021915406530791147
task implies	0.07692307692307693
Weaver ,	1.0
words ''	0.027777777777777776
major impediment	1.0
Recently	0.00043830813061582295
of knowledge-based	0.0064516129032258064
bark	0.00043830813061582295
limited world	0.5
total	0.00021915406530791147
rive	0.00021915406530791147
consider	0.0006574621959237344
, many	0.004291845493562232
Semi-supervised or	0.5
properties	0.00021915406530791147
to solving	0.010101010101010102
, as	0.02575107296137339
or tagging	0.03571428571428571
is linguistic	0.012987012987012988
semantic role	0.25
with word	0.1111111111111111
</s> Still	0.005847953216374269
`` word	0.05555555555555555
Thesaurus and	1.0
flows into	1.0
context can	0.16666666666666666
coarse-grained	0.0010957703265395574
frequent	0.00043830813061582295
-RRB- ''	0.06382978723404255
create .	1.0
57 %	1.0
as semantic	0.02564102564102564
word bass	0.034482758620689655
metaphorical	0.00021915406530791147
unnecessary	0.00021915406530791147
but comparisons	0.09090909090909091
handful	0.00021915406530791147
simple ,	0.25
in English-French	0.011363636363636364
Most research	0.5
Weaver	0.00021915406530791147
intersection	0.00021915406530791147
the `	0.010471204188481676
IR techniques	0.5
definitions .	0.16666666666666666
fish ''	0.2
impacts	0.00021915406530791147
understand	0.00021915406530791147
later	0.00043830813061582295
15 noun	1.0
of seed	0.0064516129032258064
of discussion	0.0064516129032258064
knowledge encoded	0.043478260869565216
other computer-related	0.1111111111111111
base	0.00043830813061582295
that cluster	0.03333333333333333
up	0.00021915406530791147
lists ,	0.5
machine	0.0017532325224632918
Senseval-1 -LRB-	0.5
whole	0.00021915406530791147
</s> Early	0.005847953216374269
need to	0.6666666666666666
75	0.00021915406530791147
a word	0.08695652173913043
perform state-of-the-art	0.25
improves	0.00021915406530791147
from these	0.07142857142857142
</s> In	0.07602339181286549
French	0.00043830813061582295
exploited in	1.0
purity	0.00021915406530791147
controversial .	1.0
sources provide	0.5
successful algorithms	0.25
produce	0.00021915406530791147
rather	0.00021915406530791147
is further	0.012987012987012988
techniques described	0.1111111111111111
objective	0.00021915406530791147
cases like	0.25
has	0.0035064650449265836
previously unknown	0.3333333333333333
of similarity	0.0064516129032258064
provoked	0.00021915406530791147
different from	0.0625
By	0.00021915406530791147
which only	0.045454545454545456
oldest	0.00021915406530791147
to boost	0.010101010101010102
been adopted	0.05
accuracy at	0.125
so attention	0.25
Corpora :	0.5
divide up	1.0
human annotators	0.2
hand-coded	0.00021915406530791147
give a	0.6666666666666666
be solved	0.03125
inventory ,	0.09090909090909091
application .	1.0
depend crucially	1.0
of running	0.0064516129032258064
try to	1.0
If a	1.0
annotate	0.00021915406530791147
implies	0.00021915406530791147
-LRB- indeed	0.02127659574468085
of human	0.0064516129032258064
accuracy or	0.125
popular fine-grained	0.5
word bark	0.017241379310344827
' and	0.08333333333333333
96	0.00021915406530791147
methods such	0.03333333333333333
preparing	0.00021915406530791147
Human	0.00021915406530791147
reach	0.00021915406530791147
speciﬁc evaluation	1.0
art of	0.3333333333333333
data in	0.1111111111111111
</s> Research	0.005847953216374269
ontology	0.00021915406530791147
is assumed	0.012987012987012988
an algorithm	0.09090909090909091
</s> Among	0.005847953216374269
with some	0.1111111111111111
much better	0.3333333333333333
organize different	1.0
Cambridge	0.00021915406530791147
first sentence	0.2
incoherent -RRB-	1.0
bases ,	1.0
entropy and	1.0
Ontologies	0.00021915406530791147
above	0.0010957703265395574
the context	0.010471204188481676
sentences	0.0008766162612316459
bilingual corpus	0.5
synonym	0.00021915406530791147
loose	0.00021915406530791147
the name	0.005235602094240838
superior performance	0.5
, where	0.004291845493562232
developers should	1.0
system	0.0008766162612316459
are disambiguated	0.029411764705882353
research of	0.125
sense-tagged	0.00021915406530791147
etc. .	0.6666666666666666
their polysemy	0.125
English -LRB-	0.2
exist in	0.3333333333333333
combinations	0.00021915406530791147
a small	0.043478260869565216
needed	0.0006574621959237344
greatest word	0.5
from .	0.07142857142857142
AI	0.00043830813061582295
and compared	0.011764705882352941
of language	0.0064516129032258064
, -LRB-	0.004291845493562232
a subtask	0.010869565217391304
observation	0.00021915406530791147
attention from	0.5
</s> For	0.04093567251461988
- and	1.0
compared to	0.3333333333333333
greatest challenge	0.5
researched ,	1.0
in WSD	0.022727272727272728
support vector	1.0
models	0.00021915406530791147
manual effort	1.0
Language	0.00021915406530791147
that simple	0.03333333333333333
will	0.0006574621959237344
</s> Word	0.005847953216374269
approach	0.00043830813061582295
over	0.00021915406530791147
the presence	0.005235602094240838
how	0.00021915406530791147
dominant	0.00021915406530791147
59.1	0.00021915406530791147
purposes	0.0006574621959237344
algorithm	0.0019723865877712033
the organization	0.005235602094240838
assessed	0.00021915406530791147
would	0.00021915406530791147
rive `	1.0
why research	1.0
however ,	1.0
'' task	0.06666666666666667
all clear	0.09090909090909091
Local	0.00021915406530791147
types and	0.5
substitution --	0.5
correct Recall	1.0
being used	0.2
thesauri ,	1.0
enclosure -RRB-	1.0
competitions usually	0.5
completely external	0.3333333333333333
know that	0.5
by amount	0.058823529411764705
improving	0.00021915406530791147
publicly	0.00021915406530791147
bass above	0.08333333333333333
cases	0.0008766162612316459
division of	1.0
implicit equivocation	1.0
unknown sense	1.0
some researchers	0.09090909090909091
The 2000s	0.041666666666666664
level to	0.25
is not	0.07792207792207792
high	0.00021915406530791147
Comparing	0.00021915406530791147
full lexicon	0.5
intersection algorithm	1.0
: Corpora	0.038461538461538464
</s> Inter-judge	0.005847953216374269
techniques such	0.1111111111111111
machine learning	0.5
annotators agreed	0.5
indicates	0.00021915406530791147
with earlier	0.05555555555555555
later work	0.5
Precision :	1.0
discrete sub-meanings	1.0
will not	0.3333333333333333
as the	0.07692307692307693
useful ,	1.0
hand-coding was	1.0
n surrounding	0.5
degree diversification	0.5
in 2	0.011363636363636364
be chosen	0.03125
-- has	0.16666666666666666
Moreover	0.00043830813061582295
than other	0.1111111111111111
closest induced	1.0
sense algorithm	0.020833333333333332
sampling frame	1.0
words are	0.05555555555555555
need in	0.3333333333333333
as supervised	0.02564102564102564
uses	0.00021915406530791147
identify	0.00021915406530791147
in-house ,	1.0
are two	0.029411764705882353
examples -RRB-	0.2
Here the	1.0
unknown	0.00021915406530791147
information and	0.125
resources adopted	0.14285714285714285
trends in	1.0
sense-tagged corpora	1.0
the return	0.005235602094240838
: Thesauri	0.038461538461538464
graph-based methods	0.5
purposes include	0.3333333333333333
to say	0.010101010101010102
very closely	0.14285714285714285
the most	0.031413612565445025
total word	1.0
heads ''	1.0
History	0.00021915406530791147
now	0.00021915406530791147
methods even	0.03333333333333333
The reason	0.08333333333333333
instance ,	1.0
since WSD	0.16666666666666666
large-scale lexical	1.0
decision rules	0.5
used together	0.05
as compared	0.02564102564102564
data	0.0019723865877712033
In any	0.07142857142857142
seem like	1.0
</s> Current	0.005847953216374269
so this	0.25
induction improves	0.25
disambiguated in	0.2
classifiers	0.00021915406530791147
languages	0.0006574621959237344
play '	1.0
fishing nearby	0.3333333333333333
impossible for	1.0
, especially	0.004291845493562232
difficulty of	1.0
parse meanings	1.0
These techniques	0.1
use large	0.1111111111111111
examples or	0.2
relations	0.00021915406530791147
or ,	0.03571428571428571
text by	0.1111111111111111
theoretically	0.00021915406530791147
previously	0.0006574621959237344
or cataphoras	0.03571428571428571
know	0.00043830813061582295
the different	0.010471204188481676
result clusters	0.3333333333333333
data ,	0.1111111111111111
words music	0.027777777777777776
methods .	0.03333333333333333
in corpora	0.011363636363636364
applications such	0.3333333333333333
instances	0.00043830813061582295
the problem	0.015706806282722512
however	0.0008766162612316459
data provoked	0.1111111111111111
body of	1.0
successful ,	0.25
concept	0.00043830813061582295
as in	0.05128205128205128
1998 :	0.5
require a	1.0
word-sense disambiguation	0.5
sense inventories	0.041666666666666664
information that	0.125
only the	0.4
easily	0.00021915406530791147
WordNet -RRB-	0.2222222222222222
primarily on	0.5
Bayes classifiers	1.0
and shallow	0.011764705882352941
sisters .	0.5
Lenat argue	1.0
they rely	0.16666666666666666
research community	0.125
same sense	0.2
should	0.0006574621959237344
deemed	0.00043830813061582295
rich variety	0.5
small sample	0.25
earlier experimenters	0.5
a bewildering	0.010869565217391304
understand the	1.0
until a	0.5
overcome the	1.0
coherent concept	1.0
annotators have	0.5
have shown	0.06666666666666667
1990s	0.00043830813061582295
, if	0.004291845493562232
in general	0.011363636363636364
2007 -RRB-	1.0
routinely	0.00021915406530791147
natural	0.00043830813061582295
During the	1.0
hypothesis that	1.0
words to	0.027777777777777776
-RRB- Evaluation	0.02127659574468085
which governs	0.045454545454545456
Current	0.00043830813061582295
time to	0.5
tend	0.00043830813061582295
related -LRB-	0.3333333333333333
with weights	0.05555555555555555
Language Research	1.0
especially	0.00043830813061582295
And	0.00043830813061582295
assignment for	1.0
learning will	0.08333333333333333
sequence every	1.0
presence of	1.0
repetitions	0.00021915406530791147
human languages	0.1
unanimously resolved	1.0
coarse-grained -LRB-	0.2
fact ,	1.0
2 groups	1.0
chosen target	0.3333333333333333
computer being	0.2
fine-grained sense	0.2
example ,	0.625
directly	0.00021915406530791147
other methods	0.1111111111111111
induced clusters\/senses	0.3333333333333333
induction methods	0.25
to associate	0.010101010101010102
was adopted	0.058823529411764705
in principle	0.011363636363636364
is needed	0.012987012987012988
to produce	0.010101010101010102
described here	1.0
around 95	0.5
although	0.00021915406530791147
solved by	1.0
word ,	0.034482758620689655
-LRB- 1998	0.02127659574468085
have the	0.06666666666666667
by ``	0.058823529411764705
the 1990s	0.010471204188481676
methods :	0.16666666666666666
a classifier	0.010869565217391304
was replaced	0.058823529411764705
annotators	0.00043830813061582295
very	0.0015340784571553803
methods Main	0.03333333333333333
-RRB- is	0.10638297872340426
experimenters '	1.0
you can	1.0
process	0.0006574621959237344
groups by	1.0
include Roget	0.3333333333333333
coarse-grained sense	0.2
and unlabeled	0.011764705882352941
word sense	0.20689655172413793
sense disambiguation	0.125
since 1998	0.16666666666666666
degree	0.00043830813061582295
relationships to	1.0
evaluation to	0.08333333333333333
with supervised	0.05555555555555555
corpora .	0.2
extended to	0.5
am cooking	1.0
2004	0.00021915406530791147
Comparison	0.00021915406530791147
One solution	0.25
music or	0.5
WSD exercises	0.03125
distinct senses	0.25
referred to	1.0
the implicit	0.005235602094240838
met	0.00021915406530791147
to define	0.010101010101010102
flows	0.00043830813061582295
of the	0.15483870967741936
tested .	0.3333333333333333
senses that	0.02857142857142857
's limited	0.14285714285714285
name of	1.0
it seem	0.058823529411764705
In computational	0.07142857142857142
degree ,	0.5
weights	0.00021915406530791147
approaches have	0.15789473684210525
processing :	0.3333333333333333
fine-grained word	0.2
hint	0.00021915406530791147
--	0.0013149243918474688
`` all	0.05555555555555555
Two	0.00043830813061582295
which	0.004821389436774052
by using	0.058823529411764705
loose and	1.0
text	0.0019723865877712033
, rather	0.004291845493562232
, inclusion	0.004291845493562232
a reference	0.010869565217391304
precision and	1.0
dictionaries ,	0.3333333333333333
to disambiguate	0.030303030303030304
which allows	0.045454545454545456
former sense	0.25
the decisions	0.005235602094240838
say	0.00021915406530791147
opposite	0.00021915406530791147
are subject	0.029411764705882353
small-scale ,	1.0
from 59.1	0.07142857142857142
all words	0.09090909090909091
words evergreen	0.027777777777777776
were assessed	0.16666666666666666
currently achieve	1.0
Wikipedia to	0.5
surefire decision	1.0
they	0.0013149243918474688
Thesauri Machine-readable	1.0
baseline	0.00021915406530791147
dictionary-based methods	0.3333333333333333
Knowledge is	0.3333333333333333
semantically	0.00021915406530791147
always choosing	0.3333333333333333
sense discrimination	0.020833333333333332
existence of	1.0
state-of-the	0.00021915406530791147
underlying	0.00021915406530791147
, machine	0.004291845493562232
</s> If	0.005847953216374269
sense inventory	0.16666666666666666
</s> However	0.04678362573099415
often impossible	0.16666666666666666
stoplists ,	1.0
adjectives	0.00021915406530791147
to apply	0.010101010101010102
be employed	0.03125
iterations	0.00021915406530791147
also	0.0008766162612316459
` One	0.2
language data	0.1111111111111111
sources Knowledge	0.5
now renamed	1.0
the 1980s	0.005235602094240838
have been	0.6
text need	0.1111111111111111
corpus ever	0.06666666666666667
resolution	0.00021915406530791147
annotated examples	0.3333333333333333
success	0.00043830813061582295
takes	0.00021915406530791147
, there	0.008583690987124463
thesauri	0.00043830813061582295
A set	0.25
of lexicography	0.0064516129032258064
resources -LRB-	0.14285714285714285
comprehensive body	1.0
to extract	0.010101010101010102
fleuve	0.00021915406530791147
% have	0.1111111111111111
underlying assumption	1.0
</s> One	0.005847953216374269
tagged corpora	0.3333333333333333
-LRB- at	0.02127659574468085
discourse ,	0.3333333333333333
currently	0.00021915406530791147
in WordNet	0.011363636363636364
as feature	0.02564102564102564
Cross-Lingual Evidence	1.0
database during	1.0
's algorithm	0.14285714285714285
ambiguities .	1.0
a thesaurus	0.010869565217391304
been chosen	0.05
the closest	0.005235602094240838
typically	0.00021915406530791147
go	0.00043830813061582295
context sensitive	0.16666666666666666
lexical	0.002849002849002849
computational linguistics	0.5
a sense	0.010869565217391304
disambiguating	0.0010957703265395574
disambiguation -LRB-	0.1
difference	0.00021915406530791147
frame	0.00021915406530791147
earlier	0.00043830813061582295
selectional restrictions	0.5
' task-dependency	0.08333333333333333
work ,	0.2
experimenter -LRB-	1.0
disambiguated ,	0.2
will provide	0.3333333333333333
word	0.012710935787858865
Both involve	1.0
meaning of	0.25
Thesaurus	0.00043830813061582295
, WSD	0.004291845493562232
solution	0.0006574621959237344
as -LCB-	0.02564102564102564
domain-specific setting	1.0
impediments	0.00021915406530791147
over 96	1.0
for avoiding	0.02631578947368421
the adaptation	0.005235602094240838
content words	1.0
banque `	1.0
the significance	0.005235602094240838
range of	1.0
significance	0.00021915406530791147
make the	0.25
surrounding	0.00043830813061582295
recently scientists	0.5
words with	0.027777777777777776
for evaluation	0.02631578947368421
-LRB- for	0.02127659574468085
are well-behaved	0.029411764705882353
disambiguating or	0.2
selections -RRB-	1.0
art .	0.3333333333333333
, coherence	0.004291845493562232
secondary source	1.0
to be	0.0707070707070707
unlabeled data	0.5
well as	0.3333333333333333
so	0.0008766162612316459
being trained	0.2
met only	1.0
task ,	0.07692307692307693
people can	0.5
Difficulties	0.00021915406530791147
and Wikipedia	0.011764705882352941
which a	0.045454545454545456
i.e. meaning	0.5
a given	0.03260869565217391
methods The	0.03333333333333333
arise	0.00021915406530791147
made that	1.0
public	0.00021915406530791147
in supervised	0.011363636363636364
variance .	0.5
Senseval-1	0.00043830813061582295
not divide	0.045454545454545456
History WSD	1.0
occurrences of	0.5
corpus-based	0.00021915406530791147
whereas the	1.0
Human performance	1.0
source	0.00043830813061582295
auto	0.00021915406530791147
cases ,	0.5
unannotated corpora	1.0
These methods	0.1
</s> Two	0.011695906432748537
69.0 %	1.0
and methods	0.011764705882352941
-LRB- bank	0.02127659574468085
full	0.00043830813061582295
</s> Graph-based	0.011695906432748537
accessible	0.00021915406530791147
far	0.00043830813061582295
data a	0.1111111111111111
, Bar-Hillel	0.004291845493562232
sound	0.00021915406530791147
parts ,	0.2
-- -LRB-	0.3333333333333333
and thus	0.011764705882352941
take ,	0.3333333333333333
</s> Therefore	0.011695906432748537
systems	0.0035064650449265836
figures are	1.0
secondary	0.00021915406530791147
'' -LRB-	0.13333333333333333
indicates the	1.0
those using	0.25
evaluations -LRB-	1.0
the hypothesis	0.005235602094240838
: hand-coding	0.038461538461538464
of words	0.04516129032258064
part-of-speech tagging	1.0
knowledge-based or	0.16666666666666666
did exist	1.0
results in	0.25
that samples	0.03333333333333333
facts	0.00021915406530791147
If	0.00043830813061582295
English ,	0.4
instances for	0.5
agree on	0.6666666666666666
texts	0.00021915406530791147
approaches Other	0.05263157894736842
makes	0.00021915406530791147
connectivity	0.00021915406530791147
for training	0.02631578947368421
learning optimisation	0.08333333333333333
: Structured	0.038461538461538464
dependent	0.00021915406530791147
different	0.0035064650449265836
paradigm	0.00021915406530791147
tagging algorithms	0.14285714285714285
used the	0.05
each distinct	0.08333333333333333
Advanced	0.00021915406530791147
systems developed	0.0625
, research	0.004291845493562232
a list	0.010869565217391304
These approaches	0.1
on substantial	0.03571428571428571
more expensive	0.125
examples of	0.2
disambiguated	0.0010957703265395574
sense of	0.0625
piece of	1.0
dictionary-based	0.0006574621959237344
respectively	0.00021915406530791147
document	0.00021915406530791147
, thereby	0.004291845493562232
on knowledge	0.03571428571428571
with	0.0039447731755424065
ones .	1.0
</s> While	0.005847953216374269
</s> Approaches	0.005847953216374269
evergreen	0.00021915406530791147
have used	0.06666666666666667
Sense-Tagged Corpora	1.0
linguistics ,	0.75
, Senseval-2	0.008583690987124463
largest corpus	1.0
powerful as	1.0
graph-based approaches	0.5
BNC	0.00021915406530791147
some senses	0.09090909090909091
WSD has	0.03125
these tasks	0.1111111111111111
: all-words	0.038461538461538464
inventory has	0.09090909090909091
some success	0.09090909090909091
approaches to	0.10526315789473684
computer ''	0.2
mother	0.00021915406530791147
impacts other	1.0
from Wikipedia	0.07142857142857142
reference	0.00021915406530791147
` edge	0.1
terms	0.00021915406530791147
confused	0.00021915406530791147
that use	0.03333333333333333
considerations	0.00021915406530791147
latter was	0.25
a block	0.010869565217391304
are the	0.029411764705882353
competitions	0.00043830813061582295
sentences like	0.25
usually	0.00043830813061582295
form of	1.0
such cases	0.047619047619047616
small	0.0008766162612316459
n't	0.00021915406530791147
algorithm going	0.1111111111111111
-LRB- potentially	0.02127659574468085
least	0.00043830813061582295
types	0.00043830813061582295
standard	0.00043830813061582295
shown to	0.6
measures	0.0008766162612316459
but is	0.09090909090909091
`` pine	0.05555555555555555
Other approaches	0.4
more realistic	0.25
different applications	0.0625
are very	0.029411764705882353
having	0.00043830813061582295
hand-coding	0.00021915406530791147
lists .	0.5
Almost	0.00021915406530791147
; WSD	0.25
reminiscent	0.00021915406530791147
classifiers and	1.0
the coarse-grained	0.010471204188481676
Acquisition of	1.0
typical	0.00021915406530791147
standard ,	0.5
are not	0.058823529411764705
made	0.00021915406530791147
, 8	0.004291845493562232
humans	0.00043830813061582295
due	0.00021915406530791147
example to	0.125
` flows	0.2
There	0.00021915406530791147
that a	0.03333333333333333
comparisons are	1.0
as support	0.02564102564102564
edge	0.00021915406530791147
amounts of	1.0
coarser-grained senses	1.0
sets	0.0006574621959237344
date	0.00043830813061582295
each word	0.25
levels	0.00021915406530791147
was included	0.058823529411764705
Then	0.00021915406530791147
electronic	0.00021915406530791147
those for	0.25
</s> Knowledge	0.011695906432748537
inventory .	0.18181818181818182
bank	0.00043830813061582295
named lexical	0.5
, each	0.004291845493562232
by having	0.058823529411764705
techniques and	0.1111111111111111
methods	0.006574621959237344
is primarily	0.012987012987012988
original	0.00021915406530791147
using	0.0026298487836949377
is still	0.012987012987012988
, algorithms	0.008583690987124463
being around	0.2
performed	0.00043830813061582295
dictionaries -LRB-	0.16666666666666666
It is	0.5714285714285714
relevant in	0.6666666666666666
most promising	0.1
fishing	0.0006574621959237344
does	0.00043830813061582295
by words	0.058823529411764705
again	0.00021915406530791147
Lexical sample	1.0
of n	0.0064516129032258064
subject	0.00021915406530791147
becomes much	1.0
linguistics	0.0008766162612316459
a host	0.010869565217391304
dictionaries and	0.3333333333333333
disambiguation problem	0.1
adopted .	1.0
corpora of	0.1
the systems	0.005235602094240838
the early	0.010471204188481676
modulated	0.00021915406530791147
a matter	0.010869565217391304
division	0.00043830813061582295
` river	0.1
</s> A	0.017543859649122806
, or	0.012875536480686695
the Cambridge	0.005235602094240838
to create	0.010101010101010102
knowledge base	0.08695652173913043
part of	1.0
concept :	0.5
disambiguate	0.0010957703265395574
Almost all	1.0
do n't	0.3333333333333333
relation	0.00021915406530791147
metaphorical or	1.0
statistical revolution	1.0
acquire lexical	1.0
human is	0.1
sea '	0.3333333333333333
attempt used	0.5
for the	0.10526315789473684
-LRB- written	0.02127659574468085
lexical-sample WSD	1.0
is unimportant	0.012987012987012988
methods or	0.03333333333333333
system .	0.25
boost simple	1.0
explain	0.00021915406530791147
algorithm ,	0.1111111111111111
principle infinitely	1.0
is extremely	0.012987012987012988
speech a	0.16666666666666666
decoupled is	1.0
examples	0.0010957703265395574
notion	0.00021915406530791147
dictionaries One	0.16666666666666666
achieve	0.00043830813061582295
for one	0.02631578947368421
dependent on	1.0
resolved ,	1.0
using a	0.16666666666666666
biased	0.00021915406530791147
, word	0.004291845493562232
concepts	0.00021915406530791147
thus senses	0.5
quantities	0.00021915406530791147
is routinely	0.012987012987012988
with each	0.05555555555555555
largely rule-based	1.0
complex	0.00021915406530791147
according	0.00043830813061582295
mouse '	1.0
, with	0.004291845493562232
of lexicographers	0.0064516129032258064
their dictionary	0.125
relevant to	0.3333333333333333
retrieval -LRB-	0.3333333333333333
information	0.0017532325224632918
dogs .	0.5
when the	0.3333333333333333
mothers .	1.0
researchers continue	0.2
exist	0.0006574621959237344
appropriate senses	1.0
I	0.00043830813061582295
Graph-based approaches	0.5
Approaches and	1.0
the semantic	0.005235602094240838
in one	0.022727272727272728
distinctions ,	0.4444444444444444
as well	0.02564102564102564
at the	0.4444444444444444
the human	0.015706806282722512
vector machines	1.0
, sometimes	0.004291845493562232
relations from	1.0
are normally	0.029411764705882353
each new	0.08333333333333333
In other	0.07142857142857142
Lexicographers frequently	1.0
The knowledge	0.041666666666666664
Generally ,	1.0
selections	0.00021915406530791147
1990s ,	0.5
realistic form	0.5
question	0.00021915406530791147
musical	0.0006574621959237344
all	0.002410694718387026
the	0.04185842647381109
a previously	0.010869565217391304
</s> Evaluation	0.005847953216374269
outperform	0.00021915406530791147
numbered	0.00021915406530791147
top accuracies	1.0
banque	0.00021915406530791147
differently	0.00021915406530791147
bass ''	0.16666666666666666
lexical substitution	0.15384615384615385
not	0.004821389436774052
range	0.00021915406530791147
corpus	0.003287310979618672
discussion and	1.0
seeds	0.00021915406530791147
indeterminants -LRB-	1.0
most successful	0.2
, one	0.008583690987124463
divisions	0.00021915406530791147
WSD research	0.03125
outside of	1.0
Generally	0.00021915406530791147
an assignment	0.09090909090909091
complex graph-based	1.0
a handful	0.010869565217391304
in disambiguating	0.011363636363636364
do	0.0006574621959237344
discourse and	0.3333333333333333
disambiguate such	0.2
piece	0.00021915406530791147
methods in	0.03333333333333333
type	0.00043830813061582295
objective of	1.0
meanings ,	0.25
both	0.0006574621959237344
enough to	0.5
resources ,	0.42857142857142855
of machine	0.0064516129032258064
former	0.0008766162612316459
into distinct	0.1
a body	0.010869565217391304
ontologies	0.00021915406530791147
methods As	0.03333333333333333
system If	0.25
words ,	0.1111111111111111
renamed	0.00021915406530791147
provide different	0.25
but later	0.09090909090909091
or decoupled	0.03571428571428571
an early	0.09090909090909091
its set	0.2
The Yarowsky	0.041666666666666664
writing ,	0.5
recently ,	0.5
motorcar -RCB-	1.0
decisions of	1.0
initial classifier	1.0
-LRB- fish	0.02127659574468085
word is	0.05172413793103448
them on	0.3333333333333333
discover	0.00021915406530791147
algorithms and	0.1111111111111111
</s> 15	0.005847953216374269
bottleneck since	0.25
are applicable	0.029411764705882353
return a	0.5
collocation	0.00043830813061582295
defining a	1.0
These figures	0.1
associate	0.00021915406530791147
has the	0.0625
Machines and	1.0
: Dictionary	0.038461538461538464
in such	0.011363636363636364
coarse-grained homograph	0.2
this	0.0010957703265395574
original word	1.0
take .	0.3333333333333333
real test	1.0
or more	0.03571428571428571
classified according	0.3333333333333333
-- give	0.16666666666666666
of world	0.0064516129032258064
of semantic	0.012903225806451613
to argue	0.010101010101010102
, enabling	0.004291845493562232
etc.	0.0006574621959237344
desired ,	1.0
better on	0.2
unsupervised corpus-based	0.3333333333333333
-RRB- so	0.02127659574468085
experiments	0.00021915406530791147
a training	0.010869565217391304
associate senses	1.0
only one	0.2
based on	1.0
sensitive .	1.0
'	0.0026298487836949377
pair of	1.0
context .	0.16666666666666666
The objective	0.041666666666666664
very limited	0.14285714285714285
</s> Task	0.005847953216374269
task-independent	0.00021915406530791147
the full	0.010471204188481676
requisite that	1.0
occurrence .	1.0
reason	0.00043830813061582295
competition	0.00043830813061582295
lexicographers	0.00021915406530791147
tag	0.00021915406530791147
systems ,	0.125
to specify	0.010101010101010102
A task-independent	0.25
frame was	1.0
a distinct	0.010869565217391304
governs the	1.0
, an	0.004291845493562232
impossible	0.00021915406530791147
words -LRB-	0.027777777777777776
2001	0.00021915406530791147
and explain	0.011764705882352941
cooking	0.00021915406530791147
in practice	0.022727272727272728
integrate different	1.0
distinctions	0.0019723865877712033
too	0.00021915406530791147
above ,	0.4
songs have	1.0
done in	1.0
One	0.0008766162612316459
anaphora	0.00021915406530791147
included .	1.0
1949 memorandum	1.0
some	0.002410694718387026
encodes	0.00021915406530791147
Differences	0.00021915406530791147
definitions for	0.16666666666666666
greatest	0.00043830813061582295
consider two	0.3333333333333333
-LRB- citation	0.0425531914893617
noun tasks	1.0
done	0.00021915406530791147
sense below	0.020833333333333332
thus	0.00043830813061582295
acquisition bottleneck	1.0
researchers like	0.2
an application	0.09090909090909091
the fraction	0.010471204188481676
measures .	0.25
For	0.0015340784571553803
senses relevant	0.02857142857142857
Collocation	0.00021915406530791147
sea or	0.3333333333333333
order to	1.0
senses both	0.02857142857142857
sense ;	0.020833333333333332
is incoherent	0.012987012987012988
possible parts	0.3333333333333333
numbered ``	1.0
disambiguate -RRB-	0.2
training data	0.2222222222222222
attention	0.00043830813061582295
play	0.00021915406530791147
not parse	0.045454545454545456
and 5	0.011764705882352941
be met	0.03125
both include	0.3333333333333333
of result	0.012903225806451613
tones of	1.0
The use	0.041666666666666664
research	0.0017532325224632918
until the	0.5
one do	0.06666666666666667
hoped that	1.0
Common	0.00021915406530791147
`` bass	0.2777777777777778
progressed steadily	1.0
